# app_recommend_processing_two_stage.py

import streamlit as st
import pandas as pd
import numpy as np

from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

import matplotlib.pyplot as plt
import cv2
from PIL import Image

# ==== features_pupil / GPU ÂØæÂøú =====================================
try:
    import features_pupil as fp
    if cv2.cuda.getCudaEnabledDeviceCount() == 0:
        raise ImportError("CUDA device not found")
except Exception:
    import features_pupil as fp

# ==== ÁîªÈù¢„ÉªË¶≥ÂØüË∑ùÈõ¢„Å™„Å©Ôºàfeatures_pupil Áî®Ôºâ =======================
SCREEN_W_MM = 260
DIST_MM     = 450
RES_X       = 6000
CENTER_DEG  = 2
PARAFOVEA_DEG = 5


# ==========================================
# ÁîªÂÉèÂä†Â∑•„É©„Ç§„Éñ„É©„É™ÔºàËºùÂ∫¶„Éª„Ç≥„É≥„Éà„É©„Çπ„ÉàÁ≠âÔºâ
# ==========================================
def slide_brightness(image: Image.Image, shift: float) -> Image.Image:
    img_np = np.array(image).astype("float32") / 255.0
    hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)
    hsv[:, :, 2] = np.clip(hsv[:, :, 2] + shift / 255.0, 0.0, 1.0)
    img_np = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return Image.fromarray(np.round(img_np * 255).astype("uint8"))


def adjust_contrast_adachi(image: Image.Image, scale: float) -> Image.Image:
    img_np = np.array(image)
    hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)
    hsv[:, :, 2] = cv2.convertScaleAbs(hsv[:, :, 2], alpha=scale)
    img_np = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return Image.fromarray(img_np.astype("uint8"))


def adjust_sharpness(image: Image.Image, sharpness: float) -> Image.Image:
    img_array = np.array(image)
    kernel = np.array(
        [
            [-sharpness, -sharpness, -sharpness],
            [-sharpness, 1 + 8 * sharpness, -sharpness],
            [-sharpness, -sharpness, -sharpness],
        ],
        dtype=np.float32,
    )
    img_sharpness = cv2.filter2D(img_array, -1, kernel)
    return Image.fromarray(img_sharpness)


def adjust_gamma(image: Image.Image, gamma: float) -> Image.Image:
    image = image.convert("RGB")
    gamma_correction = lambda v: int(((v / 255.0) ** gamma) * 255)
    return image.point(gamma_correction)


def stretch_rgb_clahe(image: Image.Image, clipLimit: float = 2.0, tile: int = 8) -> Image.Image:
    img_np = np.array(image).astype("float32") / 255.0
    tile = int(tile)
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(tile, tile))
    for i in range(3):
        img_np[:, :, i] = clahe.apply((img_np[:, :, i] * 255).astype("uint8")) / 255.0
    return Image.fromarray(np.round(img_np * 255).astype("uint8"))


def apply_one_op(image: Image.Image, op: str, val: float) -> Image.Image:
    if op == "brightness":
        return slide_brightness(image, shift=val)
    elif op == "contrast":
        return adjust_contrast_adachi(image, scale=val)
    elif op == "gamma":
        return adjust_gamma(image, gamma=val)
    elif op == "sharpness":
        return adjust_sharpness(image, sharpness=val)
    elif op == "equalization":
        tile = max(4, min(64, int(round(val))))
        return stretch_rgb_clahe(image, clipLimit=2.0, tile=tile)
    else:
        return image


def apply_processing_sequence(image: Image.Image, ops, vals) -> Image.Image:
    out = image.copy()
    for op, v in zip(ops, vals):
        if op is None or op == "None":
            continue
        out = apply_one_op(out, op, float(v))
    return out


# ==========================================
# „Éá„Éº„ÇøË™≠„ÅøËæº„Åø & „Éë„Éº„Çπ
# ==========================================
@st.cache_data
def load_and_parse_data(uploaded_file):
    if uploaded_file.name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    elif uploaded_file.name.endswith((".xlsx", ".xls")):
        df = pd.read_excel(uploaded_file)
    else:
        df = pd.read_csv(uploaded_file)

    def parse_params_ordered(name):
        if pd.isna(name):
            return {
                "param1": "None", "param1_val": 0.0,
                "param2": "None", "param2_val": 0.0,
                "param3": "None", "param3_val": 0.0,
            }

        clean_name = str(name).replace(".jpg", "").replace(".JPG", "")
        parts = clean_name.split("_")
        valid_ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]

        params = []
        for part in parts:
            for op in valid_ops:
                if part.startswith(op):
                    try:
                        val_str = part.replace(op, "")
                        val = float(val_str)
                        params.append((op, val))
                    except ValueError:
                        pass
                    break

        while len(params) < 3:
            params.append(("None", 0.0))

        return {
            "param1": params[0][0], "param1_val": params[0][1],
            "param2": params[1][0], "param2_val": params[1][1],
            "param3": params[2][0], "param3_val": params[2][1],
        }

    parsed_list = [parse_params_ordered(n) for n in df["image_name"]]
    params_df = pd.DataFrame(parsed_list)

    params_df["pattern_id"] = (
        params_df["param1"] + " ‚Üí " + params_df["param2"] + " ‚Üí " + params_df["param3"]
    )

    cols_to_use = params_df.columns.tolist()
    df = df.drop(columns=[c for c in cols_to_use if c in df.columns], errors="ignore")
    df_full = pd.concat([df, params_df], axis=1)
    return df_full


def create_interaction_features(df):
    valid_ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]
    X_dict = {}
    for i in range(1, 4):
        col_op = f"param{i}"
        col_val = f"param{i}_val"
        for op in valid_ops:
            mask = (df[col_op] == op).astype(float)
            X_dict[f"step{i}_{op}"] = mask * df[col_val]
    return pd.DataFrame(X_dict, index=df.index)


def compute_sample_weights(df):
    key = df["pattern_id"]
    freq = key.value_counts()
    w = key.map(freq).astype(float)
    w = 1.0 / w
    w *= len(w) / w.sum()
    return w


def generate_allowed_patterns():
    ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]
    patterns = []
    for p1 in ops:
        for p2 in ops:
            for p3 in ops:
                pat = [p1, p2, p3]

                if len(set(pat)) < 3:
                    continue
                if "brightness" in pat and p1 != "brightness":
                    continue
                if "equalization" in pat and p3 != "equalization":
                    continue
                if "brightness" in pat and "equalization" in pat:
                    continue

                patterns.append(f"{p1}_{p2}_{p3}")
    return patterns


def get_param_range(df, step, op, q_low=0.1, q_high=0.9):
    col_op = f"param{step}"
    col_val = f"param{step}_val"
    mask = df[col_op] == op

    if mask.any():
        v = df.loc[mask, col_val].astype(float)
        vmin = float(v.quantile(q_low))
        vmax = float(v.quantile(q_high))
        if vmin == vmax:
            vmin -= abs(vmin) * 0.1 + 1e-3
            vmax += abs(vmax) * 0.1 + 1e-3
    else:
        if op == "gamma":
            vmin, vmax = 0.3, 1.5
        elif op == "equalization":
            vmin, vmax = 5.0, 50.0
        elif op == "brightness":
            vmin, vmax = -50, 50
        elif op == "contrast":
            vmin, vmax = 0.5, 2.0
        else:
            vmin, vmax = 0.0, 3.0

    if op == "gamma":
        vmin = max(vmin, 0.7); vmax = min(vmax, 1.3)
    elif op == "contrast":
        vmin = max(vmin, 0.7); vmax = min(vmax, 1.3)
    elif op == "sharpness":
        vmin = max(vmin, 0.0); vmax = min(vmax, 1.5)
    elif op == "brightness":
        vmin = max(vmin, -80.0); vmax = min(vmax, 80.0)
    elif op == "equalization":
        vmin = max(vmin, 5.0); vmax = min(vmax, 40.0)

    return float(vmin), float(vmax)


def main():
    st.set_page_config(page_title="Á∏ÆÁû≥„É¢„Éá„É´‰ªò„Åç Âä†Â∑•Êé®Ëñ¶„ÉÑ„Éº„É´", layout="wide")
    st.title("üß™ ÁîªÂÉèÁâπÂæ¥ ‚Üí Á∏ÆÁû≥ ‚Üí Âä†Â∑•Êé®Ëñ¶ „ÉÑ„Éº„É´Ôºà2ÊÆµ„É¢„Éá„É´Ôºâ")

    st.sidebar.header("üìÅ „Éá„Éº„ÇøÂÖ•Âäõ")
    uploaded_file = st.sidebar.file_uploader("ÂÆüÈ®ì„Éá„Éº„Çø(CSV/Excel)", type=["csv", "xlsx", "xls"])

    if uploaded_file is None:
        st.info("üëà Â∑¶„ÅÆ„Çµ„Ç§„Éâ„Éê„Éº„Åã„Çâ„Éá„Éº„Çø„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
        return

    try:
        df_full = load_and_parse_data(uploaded_file)
    except Exception as e:
        st.error(f"„Éá„Éº„ÇøË™≠„ÅøËæº„Åø„Ç®„É©„Éº: {e}")
        return

    sample_weights = compute_sample_weights(df_full)

    tab1, tab2 = st.tabs(["üìä „Éá„Éº„ÇøÊ¶ÇË¶Å", "üß¨ Á∏ÆÁû≥„Å´Âäπ„ÅèÂä†Â∑•Êé®Ëñ¶"])

    # ===========================
    # Tab1
    # ===========================
    with tab1:
        st.subheader("„Éá„Éº„Çø„Çª„ÉÉ„ÉàÊ¶ÇË¶Å")
        st.write(f"Á∑è„Éá„Éº„ÇøÊï∞: **{len(df_full)}** Ë°å")
        st.dataframe(df_full.head())

        st.divider()
        st.subheader("Âä†Â∑•„Éë„Çø„Éº„É≥„ÅÆÂàÜÂ∏É")

        pattern_counts = df_full["pattern_id"].value_counts().sort_values(ascending=False)
        if not pattern_counts.empty:
            fig_h = max(5, len(pattern_counts) * 0.4)
            fig, ax = plt.subplots(figsize=(10, fig_h))
            bars = ax.barh(pattern_counts.index, pattern_counts.values)
            ax.set_xlabel("‰ª∂Êï∞")
            ax.grid(axis="x", linestyle="--", alpha=0.7)
            ax.set_title("pattern_id „Åî„Å®„ÅÆ‰ª∂Êï∞")
            for b in bars:
                w = b.get_width()
                ax.text(w + 1, b.get_y() + b.get_height()/2, f"{int(w)}",
                        ha="left", va="center")
            st.pyplot(fig)
        else:
            st.warning("pattern_id „Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")

        st.divider()
        st.subheader("param Âá∫ÁèæÈ†ªÂ∫¶")

        op_counts = pd.concat([
            df_full["param1"], df_full["param2"], df_full["param3"]
        ]).value_counts().rename("count")
        st.dataframe(op_counts.to_frame(), use_container_width=True)

        st.markdown("""
        **üîß „Çµ„É≥„Éó„É´Èáç„Åø**  

        - ÂêÑË°å„Å´ `pattern_id` „Çí‰ªò‰∏é„Åó„ÄÅ„Åù„ÅÆÂá∫ÁèæÂõûÊï∞„ÅÆÈÄÜÊï∞„ÇíÂ≠¶ÁøíÊôÇ„ÅÆÈáç„Åø„Å®„Åó„Å¶‰ΩøÁî®„ÄÇ  
        - È†ªÂá∫„Éë„Çø„Éº„É≥„Å†„Åë„Åß„Å™„Åè„É¨„Ç¢„Å™„Éë„Çø„Éº„É≥„ÇÇ„ÄÅ„Åß„Åç„Çã„Å†„ÅëÂÖ¨Âπ≥„Å´ÂØÑ‰∏é„Åï„Åõ„Å¶„ÅÑ„Åæ„Åô„ÄÇ
        """)

    # ===========================
    # Tab2 : 2ÊÆµ„É¢„Éá„É´
    # ===========================
    with tab2:
        st.header("üß¨ 2ÊÆµ„É¢„Éá„É´„Åß„ÅÆÁ∏ÆÁû≥Âêë„ÅçÂä†Â∑•Êé®Ëñ¶")

        # --- 0. pupilÂàó„ÅÆÈÅ∏Êäû ---
        num_cols = df_full.select_dtypes(include=[np.number]).columns.tolist()
        if not num_cols:
            st.error("Êï∞ÂÄ§Âàó„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇÁ∏ÆÁû≥Âàó„ÅåÂÖ•„Å£„Åü„Éï„Ç°„Ç§„É´„ÇíÊåáÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
            st.stop()

        default_pupil = "corrected_pupil" if "corrected_pupil" in num_cols else num_cols[0]
        pupil_col = st.selectbox(
            "Á∏ÆÁû≥„ÇíË°®„ÅôÂàóÔºà„Çø„Éº„Ç≤„ÉÉ„ÉàÔºâ",
            options=num_cols,
            index=num_cols.index(default_pupil),
        )

        direction = st.radio(
            "„Å©„Å°„Çâ„ÅÆÊñπÂêë„Åå„ÄéËâØ„ÅÑ„ÄèÔºü",
            ["ÂÄ§„ÅåÂ∞è„Åï„ÅÑ„Åª„Å©ËâØ„ÅÑÔºàÁ∏ÆÁû≥Ôºâ", "ÂÄ§„ÅåÂ§ß„Åç„ÅÑ„Åª„Å©ËâØ„ÅÑÔºàÊï£Áû≥Ôºâ"],
            index=0,
            horizontal=True,
        )
        sign = -1.0 if "Â∞è„Åï„ÅÑ" in direction else 1.0

        # --- 1. all_* ÁâπÂæ¥Èáè„ÅÆÂÄôË£ú ---
        all_cols = [c for c in df_full.columns
                    if c.startswith("all_")
                    and not c.endswith("_orig")
                    and not c.endswith("_orig_area")]
        if not all_cols:
            st.error("all_* „Å®„ÅÑ„ÅÜÂêçÂâç„ÅÆÁâπÂæ¥ÈáèÂàó„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„ÄÇ")
            st.stop()

        st.markdown(f"ÂÄôË£ú„Å®„Å™„Çã allÁ≥ªÁâπÂæ¥Èáè„ÅÆÊï∞: **{len(all_cols)}** Âàó")

        top_k = st.slider("Á∏ÆÁû≥„É¢„Éá„É´„Åß‰Ωø„ÅÜ allÁâπÂæ¥Èáè„ÅÆÊï∞ÔºàÈáçË¶ÅÂ∫¶‰∏ä‰ΩçÔºâ",
                          min_value=3, max_value=min(30, len(all_cols)),
                          value=min(10, len(all_cols)))

        n_trials_per_pattern = st.slider(
            "1„Éë„Çø„Éº„É≥„ÅÇ„Åü„Çä„ÅÆ„É©„É≥„ÉÄ„É†„Çµ„Éº„ÉÅË©¶Ë°åÊï∞",
            min_value=200, max_value=5000, value=1000, step=200
        )

        # --- Êñ∞„Åó„ÅÑÁîªÂÉè / fallback Ë°å„ÅÆÈÅ∏Êäû ---
        st.subheader("Êñ∞„Åó„ÅÑÁîªÂÉè„ÅÆÂÖ•Âäõ")
        new_image_file = st.file_uploader("Êñ∞„Åó„ÅÑÁîªÂÉè (JPEG/PNG)", type=["jpg", "jpeg", "png"], key="new_img")

        def _fmt_idx(i):
            if "image_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'image_name']}"
            elif "file_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'file_name']}"
            else:
                return str(i)

        st.markdown("ÁîªÂÉè„Çí„Ç¢„ÉÉ„Éó„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅË®ìÁ∑¥„Éá„Éº„Çø„ÅÆ1Ë°å„Çí„Äé‰ªÆ„ÅÆÊñ∞ÁîªÂÉè„Äè„Å®„Åó„Å¶‰Ωø„Åà„Åæ„Åô„ÄÇ")
        fallback_idx = st.selectbox(
            "fallback Áî®„ÅÆË°å",
            options=df_full.index,
            format_func=_fmt_idx,
        )

        if st.button("üöÄ „É¢„Éá„É´Â≠¶Áøí & Êé®Ëñ¶Âä†Â∑•Êé¢Á¥¢"):
            # -------------------------
            # 1ÊÆµÁõÆ: all_* ‚Üí pupil
            # -------------------------
            with st.spinner("1ÊÆµÁõÆ: allÁâπÂæ¥Èáè ‚Üí Á∏ÆÁû≥ „É¢„Éá„É´„ÇíÂ≠¶Áøí‰∏≠..."):
                X_img_all = df_full[all_cols].copy()
                y_pupil   = df_full[pupil_col]

                X_tr1, X_te1, y_tr1, y_te1, w_tr1, w_te1 = train_test_split(
                    X_img_all, y_pupil, sample_weights, test_size=0.2, random_state=42
                )

                rf_tmp = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
                rf_tmp.fit(X_tr1, y_tr1, sample_weight=w_tr1)

                imp_all = rf_tmp.feature_importances_
                imp_df_all = (
                    pd.DataFrame({"feature": all_cols, "importance": imp_all})
                    .sort_values("importance", ascending=False)
                    .reset_index(drop=True)
                )

                st.subheader("allÁ≥ªÁâπÂæ¥Èáè„ÅÆÈáçË¶ÅÂ∫¶ÔºàÁ∏ÆÁû≥„É¢„Éá„É´„Éª‰∫àÂÇôÂ≠¶ÁøíÔºâ")
                st.dataframe(imp_df_all.head(30), use_container_width=True)

                selected_features = imp_df_all["feature"].head(top_k).tolist()
                st.markdown("**„Åì„ÅÆ„ÅÜ„Å°‰∏ä‰Ωç k ÂÄã„Å†„Åë„Çí‰Ωø„Å£„Å¶„ÄÅÁ∏ÆÁû≥„É¢„Éá„É´„Çí‰Ωú„ÇäÁõ¥„Åó„Åæ„Åô„ÄÇ**")
                st.write("ÈÅ∏Êäû„Åï„Çå„ÅüÁâπÂæ¥Èáè:", selected_features)

                # ‰∏ä‰Ωçk„ÅßÊîπ„ÇÅ„Å¶Â≠¶Áøí
                X_img_sel = df_full[selected_features].copy()
                X_tr, X_te, y_tr, y_te, w_tr, w_te = train_test_split(
                    X_img_sel, y_pupil, sample_weights, test_size=0.2, random_state=42
                )
                rf_pupil = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)
                rf_pupil.fit(X_tr, y_tr, sample_weight=w_tr)

                r2_train1 = rf_pupil.score(X_tr, y_tr)
                r2_test1  = rf_pupil.score(X_te, y_te)

                st.subheader("1ÊÆµÁõÆ: Á∏ÆÁû≥„É¢„Éá„É´ÔºàÈÅ∏„Å∞„Çå„ÅüÁâπÂæ¥Èáè„ÅÆ„ÅøÔºâ„ÅÆÂΩì„Å¶„ÅØ„Åæ„Çä")
                st.write(f"Train R¬≤: **{r2_train1:.3f}**,  Test R¬≤: **{r2_test1:.3f}**")

                img_feature_means = X_img_sel.mean()

            # -------------------------
            # 2ÊÆµÁõÆ: (param + *_orig) ‚Üí selected allÁâπÂæ¥Èáè
            # -------------------------
            with st.spinner("2ÊÆµÁõÆ: Âä†Â∑• + ÂÖÉÁîªÂÉèÁâπÂæ¥ ‚Üí allÁâπÂæ¥Èáè „É¢„Éá„É´„ÇíÂ≠¶Áøí‰∏≠..."):
                X_param = create_interaction_features(df_full)

                orig_cols = [c for c in df_full.columns
                             if c.endswith("_orig") or c.endswith("_orig_area")]
                if orig_cols:
                    X_orig = df_full[orig_cols].copy()
                    X2 = pd.concat([X_param, X_orig], axis=1)
                else:
                    X_orig = pd.DataFrame(index=df_full.index)
                    X2 = X_param.copy()

                Y2 = df_full[selected_features].copy()

                X2_tr, X2_te, Y2_tr, Y2_te, w2_tr, w2_te = train_test_split(
                    X2, Y2, sample_weights, test_size=0.2, random_state=42
                )

                base_rf2 = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
                mo2 = MultiOutputRegressor(base_rf2)
                mo2.fit(X2_tr, Y2_tr, sample_weight=w2_tr)

                Y2_pred_te = mo2.predict(X2_te)
                r2_each2 = r2_score(Y2_te, Y2_pred_te, multioutput="raw_values")
                r2_mean2 = r2_score(Y2_te, Y2_pred_te, multioutput="uniform_average")

                st.subheader("2ÊÆµÁõÆ: allÁâπÂæ¥Èáè„É¢„Éá„É´„ÅÆÂΩì„Å¶„ÅØ„Åæ„Çä")
                r2_df2 = pd.DataFrame({"feature": selected_features, "Test_R2": r2_each2})
                st.dataframe(r2_df2, use_container_width=True)
                st.caption(f"Âπ≥Âùá Test R¬≤: **{r2_mean2:.3f}**")

                X2_means = X2.mean()

            # -------------------------
            # Êñ∞„Åó„ÅÑÁîªÂÉè„ÅÆÁâπÂæ¥Èáè & Âä†Â∑•Ââç„ÅÆÁ∏ÆÁû≥ÂÄ§
            # -------------------------
            with st.spinner("Êñ∞„Åó„ÅÑÁîªÂÉè„ÅÆÁâπÂæ¥ÈáèË®àÁÆó‰∏≠..."):
                new_image_for_display = None
                feats_before = {}

                if new_image_file is not None:
                    pil_img = Image.open(new_image_file).convert("RGB")
                    new_image_for_display = pil_img
                    img_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
                    h, w = img_bgr.shape[:2]

                    roi_masks = fp.make_masks(h, w, SCREEN_W_MM, DIST_MM, RES_X, CENTER_DEG, PARAFOVEA_DEG)
                    feats_roi = fp.compute_features_for_image(
                        img_bgr, roi_masks,
                        screen_w_mm=SCREEN_W_MM, dist_mm=DIST_MM, res_x=RES_X
                    )
                    all_masks = fp.make_all_masks()
                    feats_all = fp.compute_features_for_image(
                        img_bgr, all_masks,
                        screen_w_mm=SCREEN_W_MM, dist_mm=DIST_MM, res_x=RES_X
                    )
                    feats_before = {**feats_roi, **feats_all}

                    total_px = float(h * w)
                    area_map = {name: float(mask.sum()) / total_px for name, mask in roi_masks.items()}
                    area_map["all"] = 1.0
                else:
                    # fallback: Ë®ìÁ∑¥„Éá„Éº„Çø1Ë°å
                    feats_before = {
                        c: df_full.loc[fallback_idx, c]
                        for c in selected_features
                        if c in df_full.columns
                    }

                # „Éô„Éº„Çπ„ÅÆ allÁâπÂæ¥Èáè„Éô„ÇØ„Éà„É´
                x_before = pd.Series(index=selected_features, dtype=float)
                missing_feats = []
                for f in selected_features:
                    if f in feats_before:
                        x_before[f] = feats_before[f]
                    else:
                        missing_feats.append(f)
                        x_before[f] = np.nan

                if missing_feats:
                    st.warning(f"Êñ∞ÁîªÂÉè„Åã„ÇâÂèñÂæó„Åß„Åç„Å™„Åã„Å£„ÅüÁâπÂæ¥Èáè: {missing_feats} "
                               f"‚Üí „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂπ≥ÂùáÂÄ§„ÅßË£úÂÆå„Åó„Åæ„Åô„ÄÇ")

                x_before = x_before.fillna(img_feature_means)
                pupil_before = float(rf_pupil.predict(x_before.values.reshape(1, -1))[0])

                st.subheader("Âä†Â∑•Ââç„ÅÆ‰∫àÊ∏¨Á∏ÆÁû≥ÂÄ§")
                st.write(f"{pupil_col} „ÅÆ‰∫àÊ∏¨ÂÄ§ÔºàÂä†Â∑•ÂâçÔºâ: **{pupil_before:.3f}**")

            # -------------------------
            # 18„Éë„Çø„Éº„É≥ √ó „É©„É≥„ÉÄ„É†„Çµ„Éº„ÉÅ
            # -------------------------
            with st.spinner("18„Éë„Çø„Éº„É≥ √ó „É©„É≥„ÉÄ„É†„Çµ„Éº„ÉÅ„Åß„Éô„Çπ„ÉàÂä†Â∑•„ÇíÊé¢Á¥¢‰∏≠..."):
                allowed_patterns = generate_allowed_patterns()
                sim_records = []

                # Êñ∞ÁîªÂÉè„ÅÆ orig „Éô„ÇØ„Éà„É´ÔºàX2 Áî®Ôºâ
                orig_vec = pd.Series(index=orig_cols, dtype=float)
                if new_image_file is not None:
                    # Êó¢„Å´ roi_masks Á≠â„ÅØË®àÁÆóÊ∏à„Åø
                    # feats_before „Åã„Çâ *_orig „ÇíÂÜçË®àÁÆó„Åô„Çã„ÅÆ„ÅåÈõ£„Åó„ÅÑÂ†¥Âêà„ÅØ
                    # „Å®„Çä„ÅÇ„Åà„Åö fallback Ë°å„ÅÆ *_orig „Çí‰Ωø„ÅÜ
                    for c in orig_cols:
                        orig_vec[c] = df_full.loc[fallback_idx, c]
                else:
                    orig_vec = df_full.loc[fallback_idx, orig_cols].astype(float)

                for pat in allowed_patterns:
                    op1, op2, op3 = pat.split("_")
                    v1min, v1max = get_param_range(df_full, 1, op1)
                    v2min, v2max = get_param_range(df_full, 2, op2)
                    v3min, v3max = get_param_range(df_full, 3, op3)

                    vals1 = np.random.uniform(v1min, v1max, n_trials_per_pattern)
                    vals2 = np.random.uniform(v2min, v2max, n_trials_per_pattern)
                    vals3 = np.random.uniform(v3min, v3max, n_trials_per_pattern)

                    sim_X2 = pd.DataFrame(0.0, index=range(n_trials_per_pattern), columns=X2.columns)

                    # *_orig ÈÉ®ÂàÜ„ÅØÊñ∞ÁîªÂÉèÔºàor fallbackÔºâ„ÅÆÂÄ§„ÅßÂõ∫ÂÆö
                    for c in orig_cols:
                        sim_X2[c] = orig_vec.get(c, np.nan)
                    sim_X2 = sim_X2.fillna(X2_means)

                    c1 = f"step1_{op1}"
                    c2 = f"step2_{op2}"
                    c3 = f"step3_{op3}"
                    if c1 in sim_X2.columns:
                        sim_X2[c1] = vals1
                    if c2 in sim_X2.columns:
                        sim_X2[c2] = vals2
                    if c3 in sim_X2.columns:
                        sim_X2[c3] = vals3

                    # 2ÊÆµÁõÆ„É¢„Éá„É´„Åß allÁâπÂæ¥Èáè„Çí‰∫àÊ∏¨
                    Y_pred_feats = mo2.predict(sim_X2)  # shape: (n_trials, top_k)

                    # 1ÊÆµÁõÆ„É¢„Éá„É´„ÅßÁ∏ÆÁû≥„Çí‰∫àÊ∏¨
                    pupil_preds = rf_pupil.predict(Y_pred_feats)

                    # Score: direction „Å´Âøú„Åò„Å¶Á¨¶Âè∑ÂèçËª¢
                    scores = sign * pupil_preds

                    df_pat = pd.DataFrame(
                        {
                            "pattern": pat,
                            "Score": scores,
                            "Pupil": pupil_preds,
                            "step1_op": op1,
                            "step2_op": op2,
                            "step3_op": op3,
                            "step1_val": vals1,
                            "step2_val": vals2,
                            "step3_val": vals3,
                        }
                    )
                    # allÁâπÂæ¥Èáè„ÇÇ‰øùÂ≠òÔºàÂæå„Åß before/after „Å´‰Ωø„ÅÜÔºâ
                    for i, feat in enumerate(selected_features):
                        df_pat[feat] = Y_pred_feats[:, i]

                    sim_records.append(df_pat)

                sim_all = pd.concat(sim_records, ignore_index=True)

                # patternÂçò‰Ωç„ÅÆ„Åæ„Å®„ÇÅ
                def top5_mean(x):
                    k = max(1, int(len(x) * 0.05))
                    return x.nlargest(k).mean()

                summary = (
                    sim_all.groupby("pattern")["Score"]
                    .agg(max_score="max", top5_mean=top5_mean)
                    .reset_index()
                    .sort_values(["top5_mean", "max_score"], ascending=False)
                    .reset_index(drop=True)
                )

                st.subheader("18„Éë„Çø„Éº„É≥„ÅÆË©ï‰æ°ÁµêÊûúÔºàScoreÈ´ò„ÅÑ„Åª„Å©ËâØÔºâ")
                st.dataframe(summary.style.format({"max_score": "{:.3f}", "top5_mean": "{:.3f}"}),
                             use_container_width=True)

                # „Éô„Çπ„Éà1ÁÇπ
                best_row = sim_all.loc[sim_all["Score"].idxmax()].copy()

                pupil_after = float(best_row["Pupil"])
                delta = pupil_after - pupil_before
                ratio = np.nan if pupil_before == 0 else delta / pupil_before * 100.0

                st.divider()
                st.subheader("üëë „Åì„ÅÆÁîªÂÉè„Å´ÂØæ„Åô„Çã„Éô„Çπ„ÉàÂä†Â∑•Ê°àÔºàScore ÊúÄÂ§ßÔºâ")

                st.markdown(
                    f"- „Éë„Çø„Éº„É≥: **{best_row['pattern'].replace('_', ' ‚Üí ')}**  \n"
                    f"- Step1: **{best_row['step1_op']}** = `{best_row['step1_val']:.3f}`  \n"
                    f"- Step2: **{best_row['step2_op']}** = `{best_row['step2_val']:.3f}`  \n"
                    f"- Step3: **{best_row['step3_op']}** = `{best_row['step3_val']:.3f}`"
                )

                st.subheader("Á∏ÆÁû≥ÊåáÊ®ô„ÅÆ‰∫àÊ∏¨ÂÄ§ÔºàÂä†Â∑•Ââç vs „Éô„Çπ„ÉàÂä†Â∑•ÂæåÔºâ")
                df_pupil = pd.DataFrame(
                    {
                        "Áä∂ÊÖã": ["Âä†Â∑•Ââç", "„Éô„Çπ„ÉàÂä†Â∑•Âæå"],
                        f"‰∫àÊ∏¨ {pupil_col}": [pupil_before, pupil_after],
                        "Â§âÂåñÈáè": [np.nan, delta],
                        "Â§âÂåñÁéá[%]": [np.nan, ratio],
                    }
                )
                st.dataframe(df_pupil, use_container_width=True)

                # ÈÅ∏„Å∞„Çå„Åü allÁâπÂæ¥Èáè„ÅÆ before / after
                feat_after = [best_row[f] for f in selected_features]
                df_feats = pd.DataFrame(
                    {
                        "ÁâπÂæ¥Èáè": selected_features,
                        "Âä†Â∑•Ââç": [x_before[f] for f in selected_features],
                        "„Éô„Çπ„ÉàÂä†Â∑•Âæå": feat_after,
                    }
                )
                st.subheader("ÈáçË¶Å„Å™ allÁâπÂæ¥Èáè„ÅÆÂ§âÂåñÔºà‰∏≠Èñì„ÅÆÁâπÂæ¥ÈáèÔºâ")
                st.dataframe(df_feats, use_container_width=True)

                # ÁîªÂÉè„Çí„Ç¢„ÉÉ„Éó„Åó„Å¶„ÅÑ„Çå„Å∞ Before/After „ÇÇË°®Á§∫
                if new_image_for_display is not None:
                    st.subheader("ÁîªÂÉè„ÅÆ Before / After")
                    ops_best = [best_row["step1_op"], best_row["step2_op"], best_row["step3_op"]]
                    vals_best = [best_row["step1_val"], best_row["step2_val"], best_row["step3_val"]]
                    img_after = apply_processing_sequence(new_image_for_display, ops_best, vals_best)

                    c1, c2 = st.columns(2)
                    with c1:
                        st.image(new_image_for_display, caption="Âä†Â∑•Ââç", use_container_width=True)
                    with c2:
                        cap = (
                            f"„Éô„Çπ„ÉàÂä†Â∑•Âæå\n"
                            f"{best_row['pattern'].replace('_', ' ‚Üí ')}\n"
                            f"‰∫àÊ∏¨ {pupil_col} = {pupil_after:.3f}"
                        )
                        st.image(img_after, caption=cap, use_container_width=True)


if __name__ == "__main__":
    main()

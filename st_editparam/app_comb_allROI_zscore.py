# app_comb_allROI_zscore_gs_v2.py
# Updated: å¯è¦–åŒ–ãƒ—ãƒ­ã‚»ã‚¹è¿½åŠ  & å†å­¦ç¿’ã‚’é¿ã‘ã‚‹ãŸã‚ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†

import warnings
warnings.simplefilter("ignore")

import streamlit as st
import pandas as pd
import numpy as np

from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.model_selection import GroupKFold, KFold
from sklearn.metrics import r2_score

import matplotlib.pyplot as plt
import cv2
from PIL import Image

from xgboost import XGBRegressor

# ==== features_pupil / GPU å¯¾å¿œ =====================================
try:
    import features_pupil as fp
    if cv2.cuda.getCudaEnabledDeviceCount() == 0:
        raise ImportError("CUDA device not found")
    print("[INFO] Using GPU version (features_pupil_gpu)")
except Exception:
    import features_pupil as fp
    print("[INFO] Using CPU version (features_pupil)")

# ==== ç”»é¢ãƒ»è¦³å¯Ÿè·é›¢ãªã©ï¼ˆfeatures_pupil ç”¨ï¼‰ =======================
SCREEN_W_MM = 260
DIST_MM     = 450
RES_X       = 6000
CENTER_DEG  = 2
PARAFOVEA_DEG = 5

# ROI åã¨é‡ã¿ï¼ˆé¢ç© / ç³å­”ï¼‰
ROI_REGIONS = ("center", "parafovea", "periphery")

# é¢ç©æ¯”ï¼ˆç›®å®‰ï¼šä¸­å¿ƒ â‰’ 4%, å‚å¿ƒ â‰’ 20%, å‘¨è¾º â‰’ 76%ï¼‰
ROI_AREA_WEIGHTS = {
    "center": 0.04,
    "parafovea": 0.20,
    "periphery": 0.76,
}

# ç³å­”åæ˜ ç”¨ã®é‡ã¿ï¼ˆä»®ï¼‰
ROI_PUPIL_WEIGHTS = {
    "center": 0.5,
    "parafovea": 0.3,
    "periphery": 0.2,
}

# ç”»åƒç‰¹å¾´é‡ã¨ã—ã¦ä½¿ã‚ãªã„åˆ—ï¼ˆãƒ¡ã‚¿åˆ—ï¼‰
NON_FEATURE_COLS = [
    "folder_name",
    "å¹³å‡_å¤‰åŒ–ç‡",
    "å¹³å‡_å¤‰åŒ–é‡_z",
    "ä¸¡çœ¼.æ³¨è¦–Zåº§æ¨™[mm]",
    "pattern_id",
    "param1", "param2", "param3",
    "param1_val", "param2_val", "param3_val",
]

# ==========================================
# ç”»åƒåŠ å·¥ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆè¼åº¦ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆç­‰ï¼‰
# ==========================================
def slide_brightness(image: Image.Image, shift: float) -> Image.Image:
    img_np = np.array(image).astype("float32") / 255.0
    hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)
    hsv[:, :, 2] = np.clip(hsv[:, :, 2] + shift / 255.0, 0.0, 1.0)
    img_np = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return Image.fromarray(np.round(img_np * 255).astype("uint8"))


def adjust_contrast_adachi(image: Image.Image, scale: float) -> Image.Image:
    img_np = np.array(image)
    hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)
    hsv[:, :, 2] = cv2.convertScaleAbs(hsv[:, :, 2], alpha=scale)
    img_np = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return Image.fromarray(img_np.astype("uint8"))


def adjust_sharpness(image: Image.Image, sharpness: float) -> Image.Image:
    img_array = np.array(image)
    kernel = np.array(
        [
            [-sharpness, -sharpness, -sharpness],
            [-sharpness, 1 + 8 * sharpness, -sharpness],
            [-sharpness, -sharpness, -sharpness],
        ],
        dtype=np.float32,
    )
    img_sharpness = cv2.filter2D(img_array, -1, kernel)
    return Image.fromarray(img_sharpness)


def adjust_gamma(image: Image.Image, gamma: float) -> Image.Image:
    image = image.convert("RGB")
    gamma_correction = lambda v: int(((v / 255.0) ** gamma) * 255)
    return image.point(gamma_correction)


def stretch_rgb_clahe(image: Image.Image, clipLimit: float = 2.0, tile: int = 8) -> Image.Image:
    img_np = np.array(image).astype("float32") / 255.0
    tile = int(tile)
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(tile, tile))
    for i in range(3):
        img_np[:, :, i] = clahe.apply((img_np[:, :, i] * 255).astype("uint8")) / 255.0
    return Image.fromarray(np.round(img_np * 255).astype("uint8"))


def apply_one_op(image: Image.Image, op: str, val: float) -> Image.Image:
    if op == "brightness":
        return slide_brightness(image, shift=val)
    elif op == "contrast":
        return adjust_contrast_adachi(image, scale=val)
    elif op == "gamma":
        return adjust_gamma(image, gamma=val)
    elif op == "sharpness":
        return adjust_sharpness(image, sharpness=val)
    elif op == "equalization":
        tile = max(4, min(64, int(round(val))))
        return stretch_rgb_clahe(image, clipLimit=2.0, tile=tile)
    else:
        return image


def apply_processing_sequence(image: Image.Image, ops, vals) -> Image.Image:
    out = image.copy()
    for op, v in zip(ops, vals):
        if op is None or op == "None":
            continue
        out = apply_one_op(out, op, float(v))
    return out


# ==========================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & ãƒ‘ãƒ¼ã‚¹
# ==========================================
@st.cache_data
def load_and_parse_data(uploaded_file):
    if uploaded_file.name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    elif uploaded_file.name.endswith((".xlsx", ".xls")):
        df = pd.read_excel(uploaded_file)
    else:
        df = pd.read_csv(uploaded_file)

    def parse_params_ordered(name):
        if pd.isna(name):
            return {
                "param1": "None", "param1_val": 0.0,
                "param2": "None", "param2_val": 0.0,
                "param3": "None", "param3_val": 0.0,
            }

        clean_name = str(name).replace(".jpg", "").replace(".JPG", "")
        parts = clean_name.split("_")
        valid_ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]

        params = []
        for part in parts:
            for op in valid_ops:
                if part.startswith(op):
                    try:
                        val_str = part.replace(op, "")
                        val = float(val_str)
                        params.append((op, val))
                    except ValueError:
                        pass
                    break

        while len(params) < 3:
            params.append(("None", 0.0))

        return {
            "param1": params[0][0], "param1_val": params[0][1],
            "param2": params[1][0], "param2_val": params[1][1],
            "param3": params[2][0], "param3_val": params[2][1],
        }

    if "image_name" in df.columns:
        parsed_list = [parse_params_ordered(n) for n in df["image_name"]]
        params_df = pd.DataFrame(parsed_list)

        params_df["pattern_id"] = (
            params_df["param1"] + " â†’ " + params_df["param2"] + " â†’ " + params_df["param3"]
        )

        cols_to_use = params_df.columns.tolist()
        df = df.drop(columns=[c for c in cols_to_use if c in df.columns], errors="ignore")
        df_full = pd.concat([df, params_df], axis=1)
    else:
        df_full = df.copy()
        if "pattern_id" not in df_full.columns:
            df_full["pattern_id"] = "no_pattern"

    return df_full


def create_interaction_features(df):
    valid_ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]
    X_dict = {}
    for i in range(1, 4):
        col_op = f"param{i}"
        col_val = f"param{i}_val"
        if col_op not in df.columns or col_val not in df.columns:
            continue
        for op in valid_ops:
            mask = (df[col_op] == op).astype(float)
            X_dict[f"step{i}_{op}"] = mask * df[col_val]
    if not X_dict:
        return pd.DataFrame(index=df.index)
    return pd.DataFrame(X_dict, index=df.index)


def compute_sample_weights(df):
    key = df["pattern_id"]
    freq = key.value_counts()
    w = key.map(freq).astype(float)
    w = 1.0 / w
    w *= len(w) / w.sum()
    return w


def generate_allowed_patterns():
    ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]
    patterns = []
    for p1 in ops:
        for p2 in ops:
            for p3 in ops:
                pat = [p1, p2, p3]

                if len(set(pat)) < 3:
                    continue
                if "brightness" in pat and p1 != "brightness":
                    continue
                if "equalization" in pat and p3 != "equalization":
                    continue
                if "brightness" in pat and "equalization" in pat:
                    continue

                patterns.append(f"{p1}_{p2}_{p3}")
    return patterns


def get_param_range(df, step, op, q_low=0.1, q_high=0.9):
    col_op = f"param{step}"
    col_val = f"param{step}_val"
    if col_op not in df.columns or col_val not in df.columns:
        if op == "gamma":
            vmin, vmax = 0.7, 1.3
        elif op == "equalization":
            vmin, vmax = 5.0, 40.0
        elif op == "brightness":
            vmin, vmax = -80, 80
        elif op == "contrast":
            vmin, vmax = 0.7, 1.3
        else:
            vmin, vmax = 0.0, 1.5
        return float(vmin), float(vmax)

    mask = df[col_op] == op

    if mask.any():
        v = df.loc[mask, col_val].astype(float)
        vmin = float(v.quantile(q_low))
        vmax = float(v.quantile(q_high))
        if vmin == vmax:
            vmin -= abs(vmin) * 0.1 + 1e-3
            vmax += abs(vmax) * 0.1 + 1e-3
    else:
        if op == "gamma":
            vmin, vmax = 0.3, 1.5
        elif op == "equalization":
            vmin, vmax = 5.0, 50.0
        elif op == "brightness":
            vmin, vmax = -50, 50
        elif op == "contrast":
            vmin, vmax = 0.5, 2.0
        else:
            vmin, vmax = 0.0, 3.0

    if op == "gamma":
        vmin = max(vmin, 0.7); vmax = min(vmax, 1.3)
    elif op == "contrast":
        vmin = max(vmin, 0.7); vmax = min(vmax, 1.3)
    elif op == "sharpness":
        vmin = max(vmin, 0.0); vmax = min(vmax, 1.5)
    elif op == "brightness":
        vmin = max(vmin, -80.0); vmax = min(vmax, 80.0)
    elif op == "equalization":
        vmin = max(vmin, 5.0); vmax = min(vmax, 40.0)

    return float(vmin), float(vmax)


# ==========================
# ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒé–¢é€£
# ==========================
RF_PARAM_GRID_STAGE1 = {
    "n_estimators": [50, 75, 100, 125, 150],
    "max_depth": [5, 10, 15],
    "min_samples_leaf": [1, 3, 5],
}

XGB_PARAM_GRID_STAGE1 = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5],
    "learning_rate": [0.03, 0.06],
}

RF_PARAM_GRID_STAGE2 = {
    "n_estimators": [150, 300],
    "max_depth": [None, 8],
    "min_samples_leaf": [1, 3],
}

XGB_PARAM_GRID_STAGE2 = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5],
    "learning_rate": [0.03, 0.06],
}


def iter_param_grid(param_grid: dict):
    keys = list(param_grid.keys())
    if not keys:
        yield {}
        return
    from itertools import product
    for values in product(*[param_grid[k] for k in keys]):
        yield dict(zip(keys, values))


def create_base_regressor(model_type: str, params: dict):
    if model_type == "RandomForest":
        base_params = {
            "n_estimators": 300,
            "random_state": 42,
            "n_jobs": -1,
        }
        base_params.update(params)
        return RandomForestRegressor(**base_params)
    else:
        base_params = {
            "objective": "reg:squarederror",
            "random_state": 42,
            "n_jobs": -1,
        }
        base_params.update(params)
        return XGBRegressor(**base_params)


def grid_search_stage1(X, y, sample_weights, groups, model_type: str):
    if model_type == "RandomForest":
        param_grid = RF_PARAM_GRID_STAGE1
    else:
        param_grid = XGB_PARAM_GRID_STAGE1

    total_combo = 1
    for v in param_grid.values():
        total_combo *= len(v)

    prog = st.progress(0.0, text=f"1æ®µç›® GridSearch å®Ÿè¡Œä¸­... (0/{total_combo})")

    if groups is not None:
        splitter = GroupKFold(n_splits=5)
        def split_iter():
            return splitter.split(X, y, groups)
    else:
        splitter = KFold(n_splits=5, shuffle=True, random_state=42)
        def split_iter():
            return splitter.split(X, y)

    best_score = -np.inf
    best_params = None
    best_train_scores = None
    best_test_scores = None

    done = 0
    for params in iter_param_grid(param_grid):
        train_scores = []
        test_scores = []
        for tr_idx, te_idx in split_iter():
            X_tr = X.iloc[tr_idx]
            X_te = X.iloc[te_idx]
            y_tr = y.iloc[tr_idx]
            y_te = y.iloc[te_idx]
            w_tr = sample_weights.iloc[tr_idx]

            model = create_base_regressor(model_type, params)
            model.fit(X_tr, y_tr, sample_weight=w_tr)

            y_tr_pred = model.predict(X_tr)
            y_te_pred = model.predict(X_te)
            train_scores.append(r2_score(y_tr, y_tr_pred))
            test_scores.append(r2_score(y_te, y_te_pred))

        mean_test = float(np.mean(test_scores))
        if mean_test > best_score:
            best_score = mean_test
            best_params = params
            best_train_scores = train_scores
            best_test_scores = test_scores

        done += 1
        prog.progress(done / total_combo,
                      text=f"1æ®µç›® GridSearch å®Ÿè¡Œä¸­... ({done}/{total_combo})")

    final_model = create_base_regressor(model_type, best_params)
    final_model.fit(X, y, sample_weight=sample_weights)

    prog.progress(1.0, text="1æ®µç›® GridSearch å®Œäº† âœ…")

    cv_summary = {
        "mean_train": float(np.mean(best_train_scores)),
        "std_train": float(np.std(best_train_scores)),
        "mean_test": float(np.mean(best_test_scores)),
        "std_test": float(np.std(best_test_scores)),
    }
    return final_model, best_params, cv_summary


def grid_search_stage2(X2, Y2, sample_weights, groups, model_type: str):
    if model_type == "RandomForest":
        param_grid = RF_PARAM_GRID_STAGE2
    else:
        param_grid = XGB_PARAM_GRID_STAGE2

    total_combo = 1
    for v in param_grid.values():
        total_combo *= len(v)

    prog2 = st.progress(0.0, text=f"2æ®µç›® GridSearch å®Ÿè¡Œä¸­... (0/{total_combo})")

    if groups is not None:
        splitter = GroupKFold(n_splits=5)
        def split_iter():
            return splitter.split(X2, Y2, groups)
    else:
        splitter = KFold(n_splits=5, shuffle=True, random_state=42)
        def split_iter():
            return splitter.split(X2, Y2)

    best_score2 = -np.inf
    best_params2 = None
    best_Y2_te_all = None
    best_pred2 = None

    done2 = 0
    for params in iter_param_grid(param_grid):
        cv_scores = []
        Y2_te_list = []
        Y2_pred_list = []
        for tr_idx, te_idx in split_iter():
            X2_tr = X2.iloc[tr_idx]
            X2_te = X2.iloc[te_idx]
            Y2_tr = Y2.iloc[tr_idx]
            Y2_te = Y2.iloc[te_idx]
            w2_tr = sample_weights.iloc[tr_idx]

            base_est = create_base_regressor(model_type, params)
            mo = MultiOutputRegressor(base_est)
            mo.fit(X2_tr, Y2_tr, sample_weight=w2_tr)

            Y2_pred = mo.predict(X2_te)
            score = r2_score(Y2_te, Y2_pred, multioutput="uniform_average")
            cv_scores.append(score)

            Y2_te_list.append(Y2_te)
            Y2_pred_list.append(Y2_pred)

        mean_cv = float(np.mean(cv_scores))
        if mean_cv > best_score2:
            best_score2 = mean_cv
            best_params2 = params
            best_Y2_te_all = pd.concat(Y2_te_list, axis=0)
            best_pred2 = np.vstack(Y2_pred_list)

        done2 += 1
        prog2.progress(done2 / total_combo,
                       text=f"2æ®µç›® GridSearch å®Ÿè¡Œä¸­... ({done2}/{total_combo})")

    base_est_final = create_base_regressor(model_type, best_params2)
    mo2 = MultiOutputRegressor(base_est_final)
    mo2.fit(X2, Y2, sample_weight=sample_weights)

    r2_each2 = r2_score(best_Y2_te_all, best_pred2, multioutput="raw_values")
    r2_mean2 = best_score2

    prog2.progress(1.0, text="2æ®µç›® GridSearch å®Œäº† âœ…")

    return mo2, best_params2, r2_each2, r2_mean2


# ==== GridSearch OFF ç”¨ã®ç°¡æ˜“ãƒˆãƒ¬ãƒ¼ãƒŠ ============================
def train_stage1_fixed_params(X, y, sample_weights, groups, model_type: str, params: dict):
    prog = st.progress(0.0, text="1æ®µç›® ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...ï¼ˆGridSearch ãªã—ï¼‰")

    if groups is not None:
        splitter = GroupKFold(n_splits=5)
        splits = list(splitter.split(X, y, groups))
    else:
        splitter = KFold(n_splits=5, shuffle=True, random_state=42)
        splits = list(splitter.split(X, y))

    train_scores = []
    test_scores = []

    for i, (tr_idx, te_idx) in enumerate(splits):
        X_tr = X.iloc[tr_idx]
        X_te = X.iloc[te_idx]
        y_tr = y.iloc[tr_idx]
        y_te = y.iloc[te_idx]
        w_tr = sample_weights.iloc[tr_idx]

        model = create_base_regressor(model_type, params or {})
        model.fit(X_tr, y_tr, sample_weight=w_tr)

        y_tr_pred = model.predict(X_tr)
        y_te_pred = model.predict(X_te)
        train_scores.append(r2_score(y_tr, y_tr_pred))
        test_scores.append(r2_score(y_te, y_te_pred))

        prog.progress((i + 1) / len(splits),
                      text=f"1æ®µç›® ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...ï¼ˆ{i+1}/{len(splits)} foldï¼‰")

    final_model = create_base_regressor(model_type, params or {})
    final_model.fit(X, y, sample_weight=sample_weights)

    prog.progress(1.0, text="1æ®µç›® ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ å®Œäº† âœ…")

    cv_summary = {
        "mean_train": float(np.mean(train_scores)),
        "std_train": float(np.std(train_scores)),
        "mean_test": float(np.mean(test_scores)),
        "std_test": float(np.std(test_scores)),
    }
    return final_model, cv_summary


def train_stage2_simple(X2, Y2, sample_weights, groups, model_type: str):
    prog2 = st.progress(0.0, text="2æ®µç›® ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...ï¼ˆGridSearch ãªã—ï¼‰")

    if groups is not None:
        splitter = GroupKFold(n_splits=5)
        splits = list(splitter.split(X2, Y2, groups))
    else:
        splitter = KFold(n_splits=5, shuffle=True, random_state=42)
        splits = list(splitter.split(X2, Y2))

    cv_scores = []
    Y2_te_list = []
    Y2_pred_list = []

    for i, (tr_idx, te_idx) in enumerate(splits):
        X2_tr = X2.iloc[tr_idx]
        X2_te = X2.iloc[te_idx]
        Y2_tr = Y2.iloc[tr_idx]
        Y2_te = Y2.iloc[te_idx]
        w2_tr = sample_weights.iloc[tr_idx]

        base_est = create_base_regressor(model_type, {})
        mo = MultiOutputRegressor(base_est)
        mo.fit(X2_tr, Y2_tr, sample_weight=w2_tr)

        Y2_pred = mo.predict(X2_te)
        score = r2_score(Y2_te, Y2_pred, multioutput="uniform_average")
        cv_scores.append(score)

        Y2_te_list.append(Y2_te)
        Y2_pred_list.append(Y2_pred)

        prog2.progress((i + 1) / len(splits),
                       text=f"2æ®µç›® ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...ï¼ˆ{i+1}/{len(splits)} foldï¼‰")

    Y2_te_all = pd.concat(Y2_te_list, axis=0)
    Y2_pred_all = np.vstack(Y2_pred_list)
    r2_each2 = r2_score(Y2_te_all, Y2_pred_all, multioutput="raw_values")
    r2_mean2 = float(np.mean(cv_scores))

    base_est_final = create_base_regressor(model_type, {})
    mo2 = MultiOutputRegressor(base_est_final)
    mo2.fit(X2, Y2, sample_weight=sample_weights)

    prog2.progress(1.0, text="2æ®µç›® ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ å®Œäº† âœ…")

    return mo2, {}, r2_each2, r2_mean2


# ==== ROI â†’ all_area / all_pupilï¼ˆ1æšåˆ†ï¼‰ã®é›†ç´„ ====================
def make_weighted_globals_for_single(roi_feats: dict):
    metric_map = {}  # feat_name -> {region: value}
    for k, v in roi_feats.items():
        for r in ROI_REGIONS:
            prefix = r + "_"
            if k.startswith(prefix):
                feat_name = k[len(prefix):]
                metric_map.setdefault(feat_name, {})[r] = v
                break

    out = {}
    for feat_name, region_vals in metric_map.items():
        # area
        num_area = 0.0
        den_area = 0.0
        for r, val in region_vals.items():
            w = ROI_AREA_WEIGHTS.get(r, 0.0)
            if w == 0:
                continue
            num_area += float(val) * w
            den_area += w
        out[f"all_area_{feat_name}"] = num_area / den_area if den_area > 0 else np.nan

        # pupil
        num_pupil = 0.0
        den_pupil = 0.0
        for r, val in region_vals.items():
            w = ROI_PUPIL_WEIGHTS.get(r, 0.0)
            if w == 0:
                continue
            num_pupil += float(val) * w
            den_pupil += w
        out[f"all_pupil_{feat_name}"] = num_pupil / den_pupil if den_pupil > 0 else np.nan

    return out


# ==========================
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
# ==========================
def run_pipeline(
    df_full,
    sample_weights,
    groups,
    pupil_col,
    sign_dir,
    feat_group,
    candidate_cols,
    top_k,
    n_trials_per_pattern,
    model1_type,
    model2_type,
    use_grid1,
    use_grid2,
    new_image_file,
    fallback_idx,
):
    results = {}

    # -------------------------
    # 1æ®µç›®: ç”»åƒç‰¹å¾´ â†’ ç¸®ç³ + z ã®é‡ã¿
    # -------------------------
    with st.spinner("1æ®µç›®: ç”»åƒç‰¹å¾´ â†’ ç¸®ç³ ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­ ..."):
        X_img_all = df_full[candidate_cols].copy()
        y_pupil   = df_full[pupil_col]

        if use_grid1:
            rf_pupil_full, best_params1, cv_summary1_full = grid_search_stage1(
                X_img_all, y_pupil, sample_weights, groups, model1_type
            )
        else:
            best_params1 = {}
            rf_pupil_full, cv_summary1_full = train_stage1_fixed_params(
                X_img_all, y_pupil, sample_weights, groups, model1_type, best_params1
            )

        imp_all = rf_pupil_full.feature_importances_
        imp_df_all = (
            pd.DataFrame({"feature": candidate_cols, "importance": imp_all})
            .sort_values("importance", ascending=False)
            .reset_index(drop=True)
        )

        # ä¸Šä½kã ã‘ä½¿ç”¨
        selected_features = imp_df_all["feature"].head(top_k).tolist()

        X_img_sel = df_full[selected_features].copy()
        rf_pupil, cv_summary1_sel = train_stage1_fixed_params(
            X_img_sel, y_pupil, sample_weights, groups, model1_type, best_params1
        )

        # z ç”¨ã®é‡ã¿
        imp_sel = rf_pupil.feature_importances_
        signs = []
        for f in selected_features:
            r = df_full[f].corr(y_pupil)
            if pd.isna(r) or r == 0:
                s = 0.0
            else:
                s = sign_dir * np.sign(r)
            signs.append(s)
        signs = np.array(signs)

        w_raw = imp_sel * signs
        if np.any(w_raw != 0):
            thresh = 0.01 * np.max(np.abs(w_raw))
            w_raw[np.abs(w_raw) < thresh] = 0.0
        if np.sum(np.abs(w_raw)) > 0:
            w_raw = w_raw / np.sum(np.abs(w_raw))

        feature_weights = pd.Series(w_raw, index=selected_features)

        feat_mean = df_full[selected_features].mean()
        feat_std  = df_full[selected_features].std().replace(0, 1.0)
        img_feature_means = df_full[selected_features].mean()

    # -------------------------
    # 2æ®µç›®: (param + *_orig) â†’ selectedç‰¹å¾´é‡
    # -------------------------
    with st.spinner("2æ®µç›®: åŠ å·¥ + å…ƒç”»åƒç‰¹å¾´ â†’ é‡è¦ç‰¹å¾´é‡ ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­ ..."):
        X_param = create_interaction_features(df_full)

        orig_cols = [
            c for c in df_full.columns
            if c.endswith("_orig") or c.endswith("_orig_area") or c.endswith("_orig_pupil")
        ]
        if orig_cols:
            X_orig = df_full[orig_cols].copy()
            X2 = pd.concat([X_param, X_orig], axis=1)
        else:
            X_orig = pd.DataFrame(index=df_full.index)
            X2 = X_param.copy()

        if X2.empty:
            raise RuntimeError("paramç³»ãƒ»_origç³»ã®èª¬æ˜å¤‰æ•°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚2æ®µç›®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã€‚")

        Y2 = df_full[selected_features].copy()

        if use_grid2:
            mo2, best_params2, r2_each2, r2_mean2 = grid_search_stage2(
                X2, Y2, sample_weights, groups, model2_type
            )
        else:
            mo2, best_params2, r2_each2, r2_mean2 = train_stage2_simple(
                X2, Y2, sample_weights, groups, model2_type
            )

        X2_means = X2.mean()

    # -------------------------
    # æ–°ã—ã„ç”»åƒã®ç‰¹å¾´é‡
    # -------------------------
    with st.spinner("æ–°ã—ã„ç”»åƒã®ç‰¹å¾´é‡è¨ˆç®—ä¸­..."):
        new_image_for_display = None
        feats_before = {}

        if new_image_file is not None:
            pil_img = Image.open(new_image_file).convert("RGB")
            new_image_for_display = pil_img
            img_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
            h, w = img_bgr.shape[:2]

            roi_masks = fp.make_masks(
                h, w, SCREEN_W_MM, DIST_MM, RES_X, CENTER_DEG, PARAFOVEA_DEG
            )
            feats_roi = fp.compute_features_for_image(
                img_bgr, roi_masks,
                screen_w_mm=SCREEN_W_MM, dist_mm=DIST_MM, res_x=RES_X
            )

            all_masks = fp.make_all_masks()
            feats_all = fp.compute_features_for_image(
                img_bgr, all_masks,
                screen_w_mm=SCREEN_W_MM, dist_mm=DIST_MM, res_x=RES_X
            )

            feats_area_pupil = make_weighted_globals_for_single(feats_roi)

            feats_before = {**feats_roi, **feats_all, **feats_area_pupil}
        else:
            feats_before = {
                c: df_full.loc[fallback_idx, c]
                for c in selected_features
                if c in df_full.columns
            }

        x_before = pd.Series(index=selected_features, dtype=float)
        missing_feats = []
        for f in selected_features:
            if f in feats_before:
                x_before[f] = feats_before[f]
            else:
                missing_feats.append(f)
                x_before[f] = np.nan

        if missing_feats:
            missing_feats_list = missing_feats  # keep

        x_before = x_before.fillna(img_feature_means)
        pupil_before = float(
            rf_pupil.predict(x_before.values.reshape(1, -1))[0]
        )
        z_before = float(
            sum(
                feature_weights[f] * ((x_before[f] - feat_mean[f]) / feat_std[f])
                for f in selected_features
            )
        )

        # z ã®é€”ä¸­å¼
        z_details = []
        for f in selected_features:
            x_val = float(x_before[f])
            mu = float(feat_mean[f])
            sd = float(feat_std[f]) if feat_std[f] != 0 else 1.0
            z_i = (x_val - mu) / sd
            w_i = float(feature_weights[f])
            contrib = w_i * z_i
            z_details.append({
                "ç‰¹å¾´é‡": f,
                "å€¤ x_i": x_val,
                "å¹³å‡ Î¼_i": mu,
                "æ¨™æº–åå·® Ïƒ_i": sd,
                "æ¨™æº–åŒ– (x_i-Î¼_i)/Ïƒ_i": z_i,
                "é‡ã¿ w_i": w_i,
                "å¯„ä¸ w_i * z_i": contrib,
            })

        df_z = pd.DataFrame(z_details)
        df_z["å¯„ä¸ w_i * z_i ç´¯ç©"] = df_z["å¯„ä¸ w_i * z_i"].cumsum()

    # -------------------------
    # 18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ
    # -------------------------
    with st.spinner("18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã§ãƒ™ã‚¹ãƒˆåŠ å·¥ã‚’æ¢ç´¢ä¸­..."):
        allowed_patterns = generate_allowed_patterns()
        sim_records = []

        # orig ç‰¹å¾´
        orig_cols = [
            c for c in df_full.columns
            if c.endswith("_orig") or c.endswith("_orig_area") or c.endswith("_orig_pupil")
        ]
        orig_vec = pd.Series(index=orig_cols, dtype=float)

        if new_image_file is not None:
            for c in orig_cols:
                orig_vec[c] = df_full.loc[fallback_idx, c] if c in df_full.columns else np.nan
        else:
            if orig_cols:
                orig_vec = df_full.loc[fallback_idx, orig_cols].astype(float)

        for pat in allowed_patterns:
            op1, op2, op3 = pat.split("_")
            v1min, v1max = get_param_range(df_full, 1, op1)
            v2min, v2max = get_param_range(df_full, 2, op2)
            v3min, v3max = get_param_range(df_full, 3, op3)

            vals1 = np.random.uniform(v1min, v1max, n_trials_per_pattern)
            vals2 = np.random.uniform(v2min, v2max, n_trials_per_pattern)
            vals3 = np.random.uniform(v3min, v3max, n_trials_per_pattern)

            sim_X2 = pd.DataFrame(0.0, index=range(n_trials_per_pattern), columns=X2.columns)

            for c in orig_cols:
                sim_X2[c] = orig_vec.get(c, 0.0)
            sim_X2 = sim_X2.fillna(X2_means)

            c1 = f"step1_{op1}"
            c2 = f"step2_{op2}"
            c3 = f"step3_{op3}"
            if c1 in sim_X2.columns:
                sim_X2[c1] = vals1
            if c2 in sim_X2.columns:
                sim_X2[c2] = vals2
            if c3 in sim_X2.columns:
                sim_X2[c3] = vals3

            Y_pred_feats = mo2.predict(sim_X2)
            pupil_preds = rf_pupil.predict(Y_pred_feats)

            scores = []
            for row_idx in range(n_trials_per_pattern):
                feat_vec = pd.Series(Y_pred_feats[row_idx, :], index=selected_features)
                val_z = 0.0
                for f in selected_features:
                    x_norm = (feat_vec[f] - feat_mean[f]) / feat_std[f]
                    val_z += feature_weights[f] * x_norm
                scores.append(val_z)
            scores = np.array(scores)

            df_pat = pd.DataFrame(
                {
                    "pattern": pat,
                    "Score": scores,
                    "Pupil": pupil_preds,
                    "step1_op": op1,
                    "step2_op": op2,
                    "step3_op": op3,
                    "step1_val": vals1,
                    "step2_val": vals2,
                    "step3_val": vals3,
                }
            )
            for i, feat in enumerate(selected_features):
                df_pat[feat] = Y_pred_feats[:, i]

            sim_records.append(df_pat)

        sim_all = pd.concat(sim_records, ignore_index=True)

        def top5_mean(x):
            k = max(1, int(len(x) * 0.05))
            return x.nlargest(k).mean()

        summary = (
            sim_all.groupby("pattern")["Score"]
            .agg(max_score="max", top5_mean=top5_mean)
            .reset_index()
            .sort_values(["top5_mean", "max_score"], ascending=False)
            .reset_index(drop=True)
        )

        best_row = sim_all.loc[sim_all["Score"].idxmax()].copy()

        pupil_after = float(best_row["Pupil"])
        delta_pupil = pupil_after - pupil_before
        ratio_pupil = np.nan if pupil_before == 0 else delta_pupil / pupil_before * 100.0

        feat_after_vec = pd.Series(
            [best_row[f] for f in selected_features],
            index=selected_features,
        )
        z_after = float(
            sum(
                feature_weights[f] * ((feat_after_vec[f] - feat_mean[f]) / feat_std[f])
                for f in selected_features
            )
        )
        delta_z = z_after - z_before

    # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
    results.update(
        dict(
            df_full=df_full,
            pupil_col=pupil_col,
            sign_dir=sign_dir,
            feat_group=feat_group,
            candidate_cols=candidate_cols,
            top_k=top_k,
            n_trials_per_pattern=n_trials_per_pattern,
            model1_type=model1_type,
            model2_type=model2_type,
            use_grid1=use_grid1,
            use_grid2=use_grid2,
            rf_pupil_full=rf_pupil_full,
            best_params1=best_params1,
            cv_summary1_full=cv_summary1_full,
            imp_df_all=imp_df_all,
            selected_features=selected_features,
            rf_pupil=rf_pupil,
            cv_summary1_sel=cv_summary1_sel,
            feature_weights=feature_weights,
            feat_mean=feat_mean,
            feat_std=feat_std,
            img_feature_means=img_feature_means,
            X2=X2,
            Y2=Y2,
            mo2=mo2,
            best_params2=best_params2,
            r2_each2=r2_each2,
            r2_mean2=r2_mean2,
            X2_means=X2_means,
            orig_cols=orig_cols,
            new_image_for_display=new_image_for_display,
            x_before=x_before,
            pupil_before=pupil_before,
            z_before=z_before,
            df_z=df_z,
            sim_all=sim_all,
            summary=summary,
            best_row=best_row,
            pupil_after=pupil_after,
            delta_pupil=delta_pupil,
            ratio_pupil=ratio_pupil,
            feat_after_vec=feat_after_vec,
            z_after=z_after,
            delta_z=delta_z,
        )
    )
    return results


# ==========================
# å¯è¦–åŒ–ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
# ==========================
def render_results(results):
    df_full = results["df_full"]
    pupil_col = results["pupil_col"]

    st.markdown("### 1ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«1: ç”»åƒç‰¹å¾´ â†’ ç³å­”å¾„ï¼ˆç¸®ç³æŒ‡æ¨™ z ã®ä½œæˆï¼‰")

    st.markdown(
        """
        - ã¾ãšã€é¸æŠã—ãŸç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å€™è£œç‰¹å¾´é‡ã‚’ã™ã¹ã¦ä½¿ã£ã¦ **ç³å­”å¾„ã®å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ¢ãƒ‡ãƒ«1ï¼‰** ã‚’å­¦ç¿’ã—ã¾ã™ã€‚  
        - ãã®å¾Œã€ç‰¹å¾´é‡é‡è¦åº¦ã¨ç³å­”å¾„ã¨ã®ç›¸é–¢ã®å‘ãã‚’æ›ã‘åˆã‚ã›ã¦ **ç¬¦å·ä»˜ãã®é‡ã¿** ã‚’ä½œã‚Šã€  
          æ¨™æº–åŒ–ã—ãŸç‰¹å¾´é‡ã®é‡ã¿ä»˜ãå’Œã¨ã—ã¦ **z æŒ‡æ¨™** ã‚’å®šç¾©ã—ã¾ã™ã€‚
        """
    )

    # å…¨éƒ¨å…¥ã‚Šã®é‡è¦åº¦
    st.subheader("å€™è£œç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆå…¨éƒ¨å…¥ã‚Šãƒ¢ãƒ‡ãƒ«ï¼‰")
    st.dataframe(results["imp_df_all"].head(30), use_container_width=True)

    st.subheader("1æ®µç›® CV çµæœï¼ˆå…¨éƒ¨å…¥ã‚Šãƒ¢ãƒ‡ãƒ«ï¼‰")
    cv1 = results["cv_summary1_full"]
    st.write(f"Train RÂ²: **{cv1['mean_train']:.3f} Â± {cv1['std_train']:.3f}**")
    st.write(f"Test  RÂ²: **{cv1['mean_test']:.3f} Â± {cv1['std_test']:.3f}**")
    st.write("ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:", results["best_params1"] if results["use_grid1"] else "GridSearch OFFï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰")

    st.markdown("---")
    st.subheader(f"ä¸Šä½ {results['top_k']} ç‰¹å¾´ã®ã¿ã§å­¦ç¿’ã—ç›´ã—ãŸãƒ¢ãƒ‡ãƒ«")
    st.write("é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡:", results["selected_features"])
    cv1_sel = results["cv_summary1_sel"]
    st.write(f"Train RÂ²: **{cv1_sel['mean_train']:.3f} Â± {cv1_sel['std_train']:.3f}**")
    st.write(f"Test  RÂ²: **{cv1_sel['mean_test']:.3f} Â± {cv1_sel['std_test']:.3f}**")

    st.subheader("z ã®é‡ã¿ï¼ˆç¬¦å·ä»˜ããƒ»æ­£è¦åŒ–æ¸ˆï¼‰")
    fw = results["feature_weights"]
    df_w = pd.DataFrame({"feature": fw.index, "weight": fw.values})
    st.dataframe(df_w, use_container_width=True)

    fig, ax = plt.subplots(figsize=(6, max(4, len(fw) * 0.25)))
    ax.barh(df_w["feature"], df_w["weight"])
    ax.set_xlabel("weight")
    ax.set_title("z ã‚’æ§‹æˆã™ã‚‹ç‰¹å¾´é‡ã®é‡ã¿")
    ax.axvline(0, color="black", linewidth=1)
    st.pyplot(fig)

    st.markdown("### 2ï¸âƒ£ ãƒ¢ãƒ‡ãƒ«2: åŠ å·¥ + å…ƒç”»åƒç‰¹å¾´ â†’ é‡è¦ç‰¹å¾´é‡")

    st.markdown(
        """
        - 2æ®µç›®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®åŠ å·¥é‡ï¼ˆstep1\_gamma ãªã©ï¼‰ã¨å…ƒç”»åƒã® \*_orig ç‰¹å¾´ã‚’å…¥åŠ›ã¨ã—ã¦ã€  
          ãƒ¢ãƒ‡ãƒ«1ã§é¸ã°ã‚ŒãŸ **é‡è¦ç‰¹å¾´é‡ã®å¤‰åŒ–** ã‚’ã¾ã¨ã‚ã¦äºˆæ¸¬ã—ã¾ã™ã€‚  
        - ã“ã‚Œã«ã‚ˆã‚Šã€ä»»æ„ã®åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒ‡å®šã—ãŸã¨ãã«ã€z ãŒã©ã®æ–¹å‘ã¸å‹•ãã‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã§ãã¾ã™ã€‚
        """
    )

    r2_each2 = results["r2_each2"]
    r2_mean2 = results["r2_mean2"]
    selected_features = results["selected_features"]

    r2_df2 = pd.DataFrame({"feature": selected_features, "Test_R2": r2_each2})
    st.subheader("2æ®µç›®: ç‰¹å¾´é‡ã”ã¨ã®å½“ã¦ã¯ã¾ã‚Šï¼ˆCV ãƒ™ãƒ¼ã‚¹ï¼‰")
    st.dataframe(r2_df2, use_container_width=True)
    st.caption(f"å¹³å‡ Test RÂ²: **{r2_mean2:.3f}**")

    fig_h = max(4, len(selected_features) * 0.25)
    fig2, ax2 = plt.subplots(figsize=(8, fig_h))
    ax2.barh(r2_df2["feature"], r2_df2["Test_R2"])
    ax2.set_xlabel("Test RÂ²")
    ax2.set_title("2æ®µç›®: ç‰¹å¾´é‡ã”ã¨ã®å½“ã¦ã¯ã¾ã‚Š")
    ax2.grid(axis="x", linestyle="--", alpha=0.6)
    st.pyplot(fig2)

    # ---- ãƒ¢ãƒ‡ãƒ«2: ç‰¹å¾´é‡åˆ¥ã®é‡è¦åº¦ & ç›¸é–¢ï¼ˆå¤‰æ•°åˆ‡æ›¿ã—ã¦ã‚‚å†å­¦ç¿’ã—ãªã„ï¼‰ ----
    st.markdown("#### ãƒ¢ãƒ‡ãƒ«2ã«ãŠã‘ã‚‹ã€ŒåŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â†’ ç‰¹å¾´é‡ã€ã®é‡è¦åº¦ã¨ç›¸é–¢")

    mo2 = results["mo2"]
    X2 = results["X2"]
    Y2 = results["Y2"]

    param_cols = [c for c in X2.columns if c.startswith("step")]
    target_feat = st.selectbox(
        "è§£æå¯¾è±¡ã¨ã™ã‚‹ç‰¹å¾´é‡ï¼ˆYï¼‰",
        options=selected_features,
        key="target_feat_for_corr",
        help="ã“ã“ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’ã¯è¡Œã„ã¾ã›ã‚“ã€‚"
    )

    # å¯¾è±¡ç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    j = selected_features.index(target_feat)
    est_j = mo2.estimators_[j]
    fi = pd.Series(est_j.feature_importances_, index=X2.columns)
    fi_param = fi[param_cols].sort_values(ascending=False)

    # ç›¸é–¢ï¼ˆX ã¨å¯¾è±¡ Y ã®ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ï¼‰
    y_col = Y2[target_feat]
    corr_param = X2[fi_param.index].corrwith(y_col)

    imp_corr_df = pd.DataFrame(
        {
            "feature": fi_param.index,
            "importance": fi_param.values,
            "correlation": corr_param.values,
        }
    ).sort_values("importance", ascending=False)

    st.write("é¸æŠã—ãŸç‰¹å¾´é‡ã«å¯¾ã—ã¦ã€ã©ã®åŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã©ã‚Œãã‚‰ã„åŠ¹ã„ã¦ã„ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")
    st.dataframe(imp_corr_df.head(20), use_container_width=True)

    fig3, ax3 = plt.subplots(figsize=(8, max(4, len(imp_corr_df.head(20)) * 0.3)))
    ax3.barh(imp_corr_df["feature"].head(20), imp_corr_df["importance"].head(20))
    ax3.set_xlabel("Feature importance")
    ax3.set_title(f"ãƒ¢ãƒ‡ãƒ«2: {target_feat} ã«å¯¾ã™ã‚‹åŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é‡è¦åº¦")
    ax3.invert_yaxis()
    st.pyplot(fig3)

    fig4, ax4 = plt.subplots(figsize=(8, max(4, len(imp_corr_df.head(20)) * 0.3)))
    ax4.barh(imp_corr_df["feature"].head(20), imp_corr_df["correlation"].head(20))
    ax4.set_xlabel("Pearson correlation")
    ax4.set_title(f"ãƒ¢ãƒ‡ãƒ«2: {target_feat} ã¨åŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç›¸é–¢")
    ax4.axvline(0, color="black", linewidth=1)
    ax4.invert_yaxis()
    st.pyplot(fig4)

    # ---- æ–°ç”»åƒã«å¯¾ã™ã‚‹ z ã®è¨ˆç®—éç¨‹ ----
    st.markdown("### 3ï¸âƒ£ æ–°ã—ã„ç”»åƒï¼ˆã¾ãŸã¯ä»£è¡¨è¡Œï¼‰ã«å¯¾ã™ã‚‹ z ã®è¨ˆç®—")

    st.subheader("åŠ å·¥å‰ã®äºˆæ¸¬å€¤ã¨ z ã®å†…è¨³")
    st.write(f"{pupil_col} ã®äºˆæ¸¬å€¤ï¼ˆåŠ å·¥å‰ï¼‰: **{results['pupil_before']:.3f}**")
    st.write(f"zï¼ˆç¸®ç³ã«åŠ¹ãç‰¹å¾´ã®åˆæˆæŒ‡æ¨™, åŠ å·¥å‰ï¼‰: **{results['z_before']:.3f}**")

    st.markdown(
        r"""
        å®šç¾©:  
        \[
        z = \sum_i w_i \cdot \frac{x_i - \mu_i}{\sigma_i}
        \]
        """
    )
    st.dataframe(results["df_z"], use_container_width=True)

    # ---- 18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã®çµæœ ----
    st.markdown("### 4ï¸âƒ£ 18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã«ã‚ˆã‚‹ãƒ™ã‚¹ãƒˆåŠ å·¥æ¢ç´¢")

    st.markdown(
        """
        - brightness / contrast / gamma / sharpness / equalization ã‹ã‚‰ 3 ã¤ã‚’é¸ã³ï¼Œé †ç•ªä»˜ãã§ä¸¦ã¹ãŸ18ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”¨æ„ã€‚  
        - å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¤ã„ã¦ã€å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®šã—ãŸç¯„å›²å†…ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€  
          ãƒ¢ãƒ‡ãƒ«2ã§ç‰¹å¾´é‡ â†’ ãƒ¢ãƒ‡ãƒ«1ã§ç³å­”å¾„ãƒ»z ã‚’äºˆæ¸¬ã—ã¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚
        """
    )

    summary = results["summary"]
    st.subheader("18ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©•ä¾¡çµæœï¼ˆScore = z, å¤§ãã„ã»ã©è‰¯ï¼‰")
    st.dataframe(summary.style.format({"max_score": "{:.3f}", "top5_mean": "{:.3f}"}), use_container_width=True)

    # æ•£å¸ƒå›³: pattern ã”ã¨ã« Score vs Pupil
    st.subheader("ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®æ•£å¸ƒå›³")
    sim_all = results["sim_all"]
    fig5, ax5 = plt.subplots(figsize=(8, 6))
    for pat, g in sim_all.groupby("pattern"):
        ax5.scatter(g["Score"], g["Pupil"], s=8, alpha=0.35, label=pat)
    ax5.set_xlabel("Score (z)")
    ax5.set_ylabel("Predicted pupil")
    ax5.set_title("18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã®çµæœ")
    ax5.grid(True, linestyle="--", alpha=0.4)
    ax5.legend(bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=8)
    st.pyplot(fig5)

    # ---- ãƒ™ã‚¹ãƒˆåŠ å·¥æ¡ˆ ----
    st.markdown("### 5ï¸âƒ£ ã“ã®ç”»åƒã«å¯¾ã™ã‚‹ãƒ™ã‚¹ãƒˆåŠ å·¥æ¡ˆã¨ Before/After")

    best_row = results["best_row"]
    pupil_after = results["pupil_after"]
    delta_pupil = results["delta_pupil"]
    ratio_pupil = results["ratio_pupil"]
    z_before = results["z_before"]
    z_after = results["z_after"]
    delta_z = results["delta_z"]

    st.markdown(
        f"- **ãƒ™ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³**: `{best_row['pattern'].replace('_', ' â†’ ')}`  \n"
        f"- Step1: **{best_row['step1_op']}** = `{best_row['step1_val']:.3f}`  \n"
        f"- Step2: **{best_row['step2_op']}** = `{best_row['step2_val']:.3f}`  \n"
        f"- Step3: **{best_row['step3_op']}** = `{best_row['step3_val']:.3f}`"
    )

    df_pupil = pd.DataFrame(
        {
            "çŠ¶æ…‹": ["åŠ å·¥å‰", "ãƒ™ã‚¹ãƒˆåŠ å·¥å¾Œ"],
            f"äºˆæ¸¬ {pupil_col}": [results["pupil_before"], pupil_after],
            "å¤‰åŒ–é‡": [np.nan, delta_pupil],
            "å¤‰åŒ–ç‡[%]": [np.nan, ratio_pupil],
            "z": [z_before, z_after],
            "zå¤‰åŒ–é‡": [np.nan, delta_z],
        }
    )
    st.subheader("ç¸®ç³æŒ‡æ¨™ãƒ»z ã®äºˆæ¸¬å€¤ï¼ˆåŠ å·¥å‰ vs ãƒ™ã‚¹ãƒˆåŠ å·¥å¾Œï¼‰")
    st.dataframe(df_pupil, use_container_width=True)

    df_feats = pd.DataFrame(
        {
            "ç‰¹å¾´é‡": selected_features,
            "åŠ å·¥å‰": [results["x_before"][f] for f in selected_features],
            "ãƒ™ã‚¹ãƒˆåŠ å·¥å¾Œ": [results["feat_after_vec"][f] for f in selected_features],
            "é‡ã¿w": [results["feature_weights"][f] for f in selected_features],
        }
    )
    st.subheader("é‡è¦ãªç‰¹å¾´é‡ã®å¤‰åŒ–ï¼ˆä¸­é–“ã®ç‰¹å¾´é‡ï¼‰")
    st.dataframe(df_feats, use_container_width=True)

    # ç”»åƒ Before / After
    if results["new_image_for_display"] is not None:
        st.subheader("ç”»åƒã® Before / After")
        new_image_for_display = results["new_image_for_display"]
        ops_best = [best_row["step1_op"], best_row["step2_op"], best_row["step3_op"]]
        vals_best = [best_row["step1_val"], best_row["step2_val"], best_row["step3_val"]]
        img_after = apply_processing_sequence(new_image_for_display, ops_best, vals_best)

        c1, c2 = st.columns(2)
        with c1:
            st.image(new_image_for_display, caption="åŠ å·¥å‰", use_container_width=True)
        with c2:
            cap = (
                f"ãƒ™ã‚¹ãƒˆåŠ å·¥å¾Œ\n"
                f"{best_row['pattern'].replace('_', ' â†’ ')}\n"
                f"äºˆæ¸¬ {pupil_col} = {pupil_after:.3f}\n"
                f"z = {z_after:.3f}"
            )
            st.image(img_after, caption=cap, use_container_width=True)


# ==========================
# main
# ==========================
def main():
    st.set_page_config(page_title="ç¸®ç³ãƒ¢ãƒ‡ãƒ«ä»˜ã åŠ å·¥æ¨è–¦ãƒ„ãƒ¼ãƒ«ï¼ˆzæœ€é©åŒ–, RF/XGBï¼‰", layout="wide")

    # ãƒ•ã‚©ãƒ³ãƒˆå¤§ãã‚
    st.markdown("""
        <style>
        html, body, [class*="css"]  {
            font-size: 18px !important;
        }
        h1, h2, h3, h4 {
            font-size: 1.3em !important;
        }
        .stDataFrame div, .stMetric, .stButton>button, .stSelectbox, .stRadio, .stSlider {
            font-size: 18px !important;
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("ğŸ§ª ç”»åƒç‰¹å¾´ â†’ ç¸®ç³ â†’ åŠ å·¥æ¨è–¦ ãƒ„ãƒ¼ãƒ«ï¼ˆ2æ®µãƒ¢ãƒ‡ãƒ« + zæœ€é©åŒ–, RF/XGB + GridSearchï¼‰")

    st.sidebar.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.sidebar.file_uploader("å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿(CSV/Excel)", type=["csv", "xlsx", "xls"])

    if uploaded_file is None:
        st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    try:
        df_full = load_and_parse_data(uploaded_file)
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # ============================
    # ğŸ”§ folder_name ã®é™¤å¤–æŒ‡å®š
    # ============================
    if "folder_name" in df_full.columns:
        all_subjects = sorted(df_full["folder_name"].dropna().unique().tolist())
        excluded_subjects = st.sidebar.multiselect(
            "GroupKFold / å­¦ç¿’ã«ä½¿ã‚ãªã„ folder_name",
            options=all_subjects,
            help="ã“ã“ã§é¸ã‚“ã è¢«é¨“è€…IDã¯ã€1æ®µç›®ãƒ»2æ®µç›®ã®å­¦ç¿’ã¨CVã‹ã‚‰å®Œå…¨ã«é™¤å¤–ã•ã‚Œã¾ã™ã€‚"
        )
        if excluded_subjects:
            df_full = df_full[~df_full["folder_name"].isin(excluded_subjects)].copy()
            st.sidebar.write(f"æœ‰åŠ¹ãªè¢«é¨“è€…æ•°: {df_full['folder_name'].nunique()} / è¡Œæ•°: {len(df_full)}")
    else:
        st.sidebar.warning("folder_name åˆ—ãŒç„¡ã„ã®ã§ã€è¢«é¨“è€…é™¤å¤–ã¯ä½¿ãˆã¾ã›ã‚“ã€‚")

    # ===== GroupKFold ç”¨ group åˆ—ã®è¨­å®š =====
    st.sidebar.subheader("ğŸ§ª ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š")

    use_groupkfold = st.sidebar.checkbox(
        "GroupKFold ã‚’ä½¿ã†ï¼ˆåŒã˜groupã‚’åŒä¸€foldã«å…¥ã‚Œãªã„ï¼‰",
        value=("folder_name" in df_full.columns),
        help="OFFã«ã™ã‚‹ã¨é€šå¸¸ã®KFoldã«ãªã‚Šã¾ã™ã€‚"
    )

    group_col = None
    groups = None

    if use_groupkfold:
        candidate_group_cols = []
        for c in df_full.columns:
            nunique = df_full[c].nunique(dropna=True)
            if 1 < nunique < len(df_full):
                candidate_group_cols.append(c)

        if not candidate_group_cols:
            st.sidebar.warning("GroupKFold ã«ä½¿ãˆãã†ãªåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã®ã§ã€é€šå¸¸ã® KFold ã‚’ä½¿ã„ã¾ã™ã€‚")
            use_groupkfold = False
            groups = None
        else:
            default_idx = 0
            if "folder_name" in candidate_group_cols:
                default_idx = candidate_group_cols.index("folder_name")

            group_col = st.sidebar.selectbox(
                "GroupKFold ã«ä½¿ã†åˆ—",
                options=candidate_group_cols,
                index=default_idx,
                help="ä¾‹ï¼šfolder_nameï¼ˆè¢«é¨“è€…IDï¼‰ãªã©"
            )
            groups = df_full[group_col]
    else:
        groups = None

    sample_weights = compute_sample_weights(df_full)

    tab1, tab2 = st.tabs(["ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦", "ğŸ§¬ ç¸®ç³ã«åŠ¹ãåŠ å·¥æ¨è–¦ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å¯è¦–åŒ–ä»˜ãï¼‰"])

    # ===========================
    # Tab1
    # ===========================
    with tab1:
        st.subheader("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦")
        st.write(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: **{len(df_full)}** è¡Œ")
        st.dataframe(df_full.head(), use_container_width=True)

        st.divider()
        st.subheader("åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†å¸ƒ")

        if "pattern_id" in df_full.columns:
            pattern_counts = df_full["pattern_id"].value_counts().sort_values(ascending=False)
        else:
            pattern_counts = pd.Series([], dtype=int)

        if not pattern_counts.empty:
            fig_h = max(5, len(pattern_counts) * 0.4)
            fig, ax = plt.subplots(figsize=(10, fig_h))
            bars = ax.barh(pattern_counts.index, pattern_counts.values)
            ax.set_xlabel("ä»¶æ•°")
            ax.grid(axis="x", linestyle="--", alpha=0.7)
            ax.set_title("pattern_id ã”ã¨ã®ä»¶æ•°")
            for b in bars:
                w = b.get_width()
                ax.text(w + 1, b.get_y() + b.get_height()/2, f"{int(w)}",
                        ha="left", va="center")
            st.pyplot(fig)
        else:
            st.warning("pattern_id ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        if {"param1", "param2", "param3"}.issubset(df_full.columns):
            st.divider()
            st.subheader("param å‡ºç¾é »åº¦")
            op_counts = pd.concat([
                df_full["param1"], df_full["param2"], df_full["param3"]
            ]).value_counts().rename("count")
            st.dataframe(op_counts.to_frame(), use_container_width=True)

        st.markdown("""
        **ğŸ”§ ã‚µãƒ³ãƒ—ãƒ«é‡ã¿**  

        - å„è¡Œã« `pattern_id` ã‚’ä»˜ä¸ã—ã€ãã®å‡ºç¾å›æ•°ã®é€†æ•°ã‚’å­¦ç¿’æ™‚ã®é‡ã¿ã¨ã—ã¦ä½¿ç”¨ã€‚  
        - é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘ã§ãªããƒ¬ã‚¢ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã€ã§ãã‚‹ã ã‘å…¬å¹³ã«å¯„ä¸ã•ã›ã¦ã„ã¾ã™ã€‚
        """)

    # ===========================
    # Tab2
    # ===========================
    with tab2:
        st.header("ğŸ§¬ 2æ®µãƒ¢ãƒ‡ãƒ«ã§ã®ç¸®ç³å‘ãåŠ å·¥æ¨è–¦ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å¯è¦–åŒ–ä»˜ãï¼‰")

        num_cols_all = df_full.select_dtypes(include=[np.number]).columns.tolist()
        if not num_cols_all:
            st.error("æ•°å€¤åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç¸®ç³åˆ—ãŒå…¥ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        default_pupil = "corrected_pupil" if "corrected_pupil" in num_cols_all else num_cols_all[0]
        pupil_col = st.selectbox(
            "ç¸®ç³ã‚’è¡¨ã™åˆ—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰",
            options=num_cols_all,
            index=num_cols_all.index(default_pupil),
        )

        direction = st.radio(
            "ã©ã¡ã‚‰ã®æ–¹å‘ãŒã€è‰¯ã„ã€ï¼Ÿ",
            ["å€¤ãŒå°ã•ã„ã»ã©è‰¯ã„ï¼ˆç¸®ç³ï¼‰", "å€¤ãŒå¤§ãã„ã»ã©è‰¯ã„ï¼ˆæ•£ç³ï¼‰"],
            index=0,
            horizontal=True,
        )
        sign_dir = -1.0 if "å°ã•ã„" in direction else 1.0

        feat_group = st.radio(
            "1æ®µç›®ã§ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—",
            ["all", "all_area", "all_pupil", "ROI"],
            index=0,
            horizontal=True,
        )

        if feat_group == "all":
            candidate_cols = [
                c for c in num_cols_all
                if c.startswith("all_")
                and not c.startswith("all_area_")
                and not c.startswith("all_pupil_")
                and not c.endswith("_orig")
                and c not in NON_FEATURE_COLS
                and c != pupil_col
            ]
        elif feat_group == "all_area":
            candidate_cols = [
                c for c in num_cols_all
                if c.startswith("all_area_")
                and not c.endswith("_orig")
                and c not in NON_FEATURE_COLS
                and c != pupil_col
            ]
        elif feat_group == "all_pupil":
            candidate_cols = [
                c for c in num_cols_all
                if c.startswith("all_pupil_")
                and not c.endswith("_orig")
                and c not in NON_FEATURE_COLS
                and c != pupil_col
            ]
        else:  # ROI
            candidate_cols = [
                c for c in num_cols_all
                if (
                    c.startswith("center_")
                    or c.startswith("parafovea_")
                    or c.startswith("periphery_")
                )
                and "_orig" not in c
                and c not in NON_FEATURE_COLS
                and c != pupil_col
            ]

        if not candidate_cols:
            st.error("å€™è£œã¨ãªã‚‹ç‰¹å¾´é‡åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—åè¦å‰‡ã‚„ NON_FEATURE_COLS ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        st.markdown(f"**ã‚°ãƒ«ãƒ¼ãƒ—: {feat_group}** / å€™è£œç‰¹å¾´é‡ã®æ•°: **{len(candidate_cols)}** åˆ—")

        top_k = st.slider(
            "ç¸®ç³ãƒ¢ãƒ‡ãƒ«ã§ä½¿ã†ç‰¹å¾´é‡ã®æ•°ï¼ˆé‡è¦åº¦ä¸Šä½ï¼‰",
            min_value=3, max_value=min(30, len(candidate_cols)),
            value=min(10, len(candidate_cols)),
        )

        n_trials_per_pattern = st.slider(
            "1ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚ãŸã‚Šã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒè©¦è¡Œæ•°",
            min_value=200, max_value=5000, value=1000, step=200
        )

        model1_type = st.radio(
            "1æ®µç›®ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆç”»åƒç‰¹å¾´ â†’ ç¸®ç³ï¼‰",
            ["RandomForest", "XGBoost"],
            index=0,
            horizontal=True,
        )
        model2_type = st.radio(
            "2æ®µç›®ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆåŠ å·¥+*_orig â†’ ç”»åƒç‰¹å¾´ï¼‰",
            ["RandomForest", "XGBoost"],
            index=0,
            horizontal=True,
        )

        use_grid1 = st.checkbox("1æ®µç›®ã§ GridSearch ã‚’ä½¿ã†", value=True)
        use_grid2 = st.checkbox("2æ®µç›®ã§ GridSearch ã‚’ä½¿ã†", value=True)

        # æ–°ç”»åƒå…¥åŠ›
        st.subheader("æ–°ã—ã„ç”»åƒã®å…¥åŠ›")
        new_image_file = st.file_uploader("æ–°ã—ã„ç”»åƒ (JPEG/PNG)", type=["jpg", "jpeg", "png"], key="new_img")

        def _fmt_idx(i):
            if "image_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'image_name']}"
            elif "file_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'file_name']}"
            else:
                return str(i)

        st.markdown("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ã—ãªã„å ´åˆã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®1è¡Œã‚’ã€ä»®ã®æ–°ç”»åƒã€ã¨ã—ã¦ä½¿ãˆã¾ã™ã€‚")
        fallback_idx = st.selectbox(
            "fallback ç”¨ã®è¡Œ",
            options=df_full.index,
            format_func=_fmt_idx,
        )

        # ----------------------
        # å®Ÿè¡Œãƒœã‚¿ãƒ³ & ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        # ----------------------
        run_clicked = st.button("ğŸš€ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & æ¨è–¦åŠ å·¥æ¢ç´¢ã‚’å®Ÿè¡Œ / æ›´æ–°")

        key_name = "app_comb_allROI_results"

        if run_clicked:
            try:
                results = run_pipeline(
                    df_full=df_full,
                    sample_weights=sample_weights,
                    groups=groups,
                    pupil_col=pupil_col,
                    sign_dir=sign_dir,
                    feat_group=feat_group,
                    candidate_cols=candidate_cols,
                    top_k=top_k,
                    n_trials_per_pattern=n_trials_per_pattern,
                    model1_type=model1_type,
                    model2_type=model2_type,
                    use_grid1=use_grid1,
                    use_grid2=use_grid2,
                    new_image_file=new_image_file,
                    fallback_idx=fallback_idx,
                )
                st.session_state[key_name] = results
            except Exception as e:
                st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                return

        if key_name in st.session_state:
            st.info("â–¶ å³å´ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’åˆ‡ã‚Šæ›¿ãˆã¦ã‚‚å†å­¦ç¿’ã¯è¡Œã‚ãšã€ä¿å­˜æ¸ˆã¿ã®çµæœã‚’ä½¿ã£ã¦å¯è¦–åŒ–ã—ã¦ã„ã¾ã™ã€‚")
            render_results(st.session_state[key_name])
        else:
            st.warning("ã¾ãšã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ & æ¨è–¦åŠ å·¥æ¢ç´¢ã‚’å®Ÿè¡Œ / æ›´æ–°ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦çµæœã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()

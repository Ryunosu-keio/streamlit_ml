# app_recommend_processing.py

import streamlit as st
import pandas as pd
import numpy as np

from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

import matplotlib.pyplot as plt
import cv2
from PIL import Image

# ==== features_pupil / GPU å¯¾å¿œ =====================================
try:
    import features_pupil as fp
    if cv2.cuda.getCudaEnabledDeviceCount() == 0:
        raise ImportError("CUDA device not found")
except Exception:
    import features_pupil as fp

# ==== ç”»é¢ãƒ»è¦³å¯Ÿè·é›¢ãªã©ï¼ˆfeatures_pupil ç”¨ï¼‰ =======================
SCREEN_W_MM = 260
DIST_MM     = 450
RES_X       = 6000
CENTER_DEG  = 2
PARAFOVEA_DEG = 5


# ==========================================
# ç”»åƒåŠ å·¥ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆè¼åº¦ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆç­‰ï¼‰
# ==========================================
def slide_brightness(image: Image.Image, shift: float) -> Image.Image:
    """HSV ã® V ãƒãƒ£ãƒ³ãƒãƒ«ã«ã‚·ãƒ•ãƒˆã‚’åŠ ãˆã¦è¼åº¦ã‚’èª¿æ•´"""
    img_np = np.array(image).astype("float32") / 255.0
    hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)
    hsv[:, :, 2] = np.clip(hsv[:, :, 2] + shift / 255.0, 0.0, 1.0)
    img_np = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return Image.fromarray(np.round(img_np * 255).astype("uint8"))


def adjust_contrast_adachi(image: Image.Image, scale: float) -> Image.Image:
    """V ãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã‚’ä¼¸é•·"""
    img_np = np.array(image)
    hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)
    hsv[:, :, 2] = cv2.convertScaleAbs(hsv[:, :, 2], alpha=scale)
    img_np = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return Image.fromarray(img_np.astype("uint8"))


def adjust_sharpness(image: Image.Image, sharpness: float) -> Image.Image:
    """3x3 ã®é‹­åˆ©åŒ–ãƒ•ã‚£ãƒ«ã‚¿"""
    img_array = np.array(image)
    kernel = np.array(
        [
            [-sharpness, -sharpness, -sharpness],
            [-sharpness, 1 + 8 * sharpness, -sharpness],
            [-sharpness, -sharpness, -sharpness],
        ],
        dtype=np.float32,
    )
    img_sharpness = cv2.filter2D(img_array, -1, kernel)
    return Image.fromarray(img_sharpness)


def adjust_gamma(image: Image.Image, gamma: float) -> Image.Image:
    """ã‚¬ãƒ³ãƒè£œæ­£ (value^gamma)"""
    image = image.convert("RGB")
    gamma_correction = lambda v: int(((v / 255.0) ** gamma) * 255)
    return image.point(gamma_correction)


def stretch_rgb_clahe(image: Image.Image, clipLimit: float = 2.0, tile: int = 8) -> Image.Image:
    """RGB å„ãƒãƒ£ãƒ³ãƒãƒ«ã« CLAHEï¼ˆå±€æ‰€ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å¦åŒ–ï¼‰"""
    img_np = np.array(image).astype("float32") / 255.0
    tile = int(tile)
    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=(tile, tile))
    for i in range(3):
        img_np[:, :, i] = clahe.apply((img_np[:, :, i] * 255).astype("uint8")) / 255.0
    return Image.fromarray(np.round(img_np * 255).astype("uint8"))


def apply_one_op(image: Image.Image, op: str, val: float) -> Image.Image:
    """1 ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®åŠ å·¥ã‚’é©ç”¨"""
    if op == "brightness":
        return slide_brightness(image, shift=val)
    elif op == "contrast":
        return adjust_contrast_adachi(image, scale=val)
    elif op == "gamma":
        return adjust_gamma(image, gamma=val)
    elif op == "sharpness":
        return adjust_sharpness(image, sharpness=val)
    elif op == "equalization":
        # ã“ã“ã§ã¯ val ã‚’ã‚¿ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã—ã¦è§£é‡ˆï¼ˆã–ã£ãã‚Šï¼‰
        tile = max(4, min(64, int(round(val))))
        return stretch_rgb_clahe(image, clipLimit=2.0, tile=tile)
    else:
        return image


def apply_processing_sequence(image: Image.Image, ops, vals) -> Image.Image:
    """(op1, op2, op3) ã‚’é †ç•ªã«é©ç”¨"""
    out = image.copy()
    for op, v in zip(ops, vals):
        if op is None or op == "None":
            continue
        out = apply_one_op(out, op, float(v))
    return out


# ==========================================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & ãƒ‘ãƒ¼ã‚¹å‡¦ç† (å…±é€š)
# ==========================================
@st.cache_data
def load_and_parse_data(uploaded_file):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰é †åºã¨å€¤ã‚’æŠ½å‡ºã—ã¦æ§‹é€ åŒ–ã™ã‚‹"""
    # æ‹¡å¼µå­åˆ¤åˆ¥
    if uploaded_file.name.endswith(".csv"):
        df = pd.read_csv(uploaded_file)
    elif uploaded_file.name.endswith((".xlsx", ".xls")):
        df = pd.read_excel(uploaded_file)
    else:
        df = pd.read_csv(uploaded_file)

    def parse_params_ordered(name):
        if pd.isna(name):
            return {
                "param1": "None",
                "param1_val": 0.0,
                "param2": "None",
                "param2_val": 0.0,
                "param3": "None",
                "param3_val": 0.0,
            }

        clean_name = str(name).replace(".jpg", "").replace(".JPG", "")
        parts = clean_name.split("_")
        valid_ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]

        params = []
        for part in parts:
            for op in valid_ops:
                if part.startswith(op):
                    try:
                        val_str = part.replace(op, "")
                        val = float(val_str)
                        params.append((op, val))
                    except ValueError:
                        continue
                    break

        while len(params) < 3:
            params.append(("None", 0.0))

        return {
            "param1": params[0][0],
            "param1_val": params[0][1],
            "param2": params[1][0],
            "param2_val": params[1][1],
            "param3": params[2][0],
            "param3_val": params[2][1],
        }

    parsed_list = [parse_params_ordered(n) for n in df["image_name"]]
    params_df = pd.DataFrame(parsed_list)

    # é †åºãƒ‘ã‚¿ãƒ¼ãƒ³ID (ä¾‹: gamma -> sharpness -> equalization)
    params_df["pattern_id"] = (
        params_df["param1"] + " â†’ " + params_df["param2"] + " â†’ " + params_df["param3"]
    )

    # é‡è¤‡åˆ—å‰Šé™¤
    cols_to_use = params_df.columns.tolist()
    df = df.drop(columns=[c for c in cols_to_use if c in df.columns], errors="ignore")

    df_full = pd.concat([df, params_df], axis=1)
    return df_full


# ==========================================
# 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° & é‡ã¿è¨ˆç®—
# ==========================================
def create_interaction_features(df):
    """
    'step1_gamma' ã®ã‚ˆã†ã«ã€ã€Œå ´æ‰€Ã—ç¨®é¡ã€ã§å€¤ã‚’æ ¼ç´ã™ã‚‹ç‰¹å¾´é‡ã‚’ä½œæˆã€‚
    ã“ã‚Œã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ã¯ã€Œ1æ‰‹ç›®ã®Gammaã€ã¨ã€Œ2æ‰‹ç›®ã®Gammaã€ã‚’åŒºåˆ¥ã§ãã‚‹ã€‚
    """
    valid_ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]
    X_dict = {}

    for i in range(1, 4):
        col_op = f"param{i}"
        col_val = f"param{i}_val"

        for op in valid_ops:
            mask = (df[col_op] == op).astype(float)
            X_dict[f"step{i}_{op}"] = mask * df[col_val]

    return pd.DataFrame(X_dict, index=df.index)


def compute_sample_weights(df):
    """
    pattern_id ã”ã¨ã«ä»¶æ•°ã‚’æ•°ãˆã€ãã®é€†æ•°ã‚’é‡ã¿ã¨ã™ã‚‹ã€‚
    -> å¤šãå«ã¾ã‚Œã‚‹åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å­¦ç¿’ãŒåã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚
    """
    key = df["pattern_id"]
    freq = key.value_counts()
    w = key.map(freq).astype(float)
    w = 1.0 / w
    w *= len(w) / w.sum()  # å¹³å‡1ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    return w


def generate_allowed_patterns():
    """
    brightnessã¯æœ€åˆ / equalizationã¯æœ€å¾Œ / brightnessã¨equalizationã¯åŒå±…ã—ãªã„ / é‡è¤‡ãªã—
    ã‚’æº€ãŸã™å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæƒ³å®š18é€šã‚Šï¼‰ã‚’åˆ—æŒ™ã€‚
    æ–‡å­—åˆ—è¡¨ç¾ã¯ 'op1_op2_op3'
    """
    ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]
    patterns = []

    for p1 in ops:
        for p2 in ops:
            for p3 in ops:
                pat = [p1, p2, p3]

                if len(set(pat)) < 3:
                    continue

                if "brightness" in pat and p1 != "brightness":
                    continue

                if "equalization" in pat and p3 != "equalization":
                    continue

                if "brightness" in pat and "equalization" in pat:
                    continue

                patterns.append(f"{p1}_{p2}_{p3}")

    return patterns  # 18å€‹ã«ãªã‚‹ã¯ãš


def get_param_range(df, step, op, q_low=0.1, q_high=0.9):
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ stepÃ—op ã”ã¨ã®å€¤ã®åˆ†ä½ç‚¹ç¯„å›²ã‚’å–ã‚Šã€
    ã•ã‚‰ã«æ“ä½œã”ã¨ã®å®‰å…¨ç¯„å›²ã§ã‚¯ãƒªãƒƒãƒ—ã™ã‚‹ã€‚
    """
    col_op = f"param{step}"
    col_val = f"param{step}_val"
    mask = df[col_op] == op

    if mask.any():
        v = df.loc[mask, col_val].astype(float)
        vmin = float(v.quantile(q_low))
        vmax = float(v.quantile(q_high))
        if vmin == vmax:
            vmin -= abs(vmin) * 0.1 + 1e-3
            vmax += abs(vmax) * 0.1 + 1e-3
    else:
        if op == "gamma":
            vmin, vmax = 0.3, 1.5
        elif op == "equalization":
            vmin, vmax = 5.0, 50.0
        elif op == "brightness":
            vmin, vmax = -50, 50
        elif op == "contrast":
            vmin, vmax = 0.5, 2.0
        else:
            vmin, vmax = 0.0, 3.0

    # å®‰å…¨ã‚¯ãƒªãƒƒãƒ—ï¼ˆã‚„ã‚Šéãé˜²æ­¢ï¼‰
    if op == "gamma":
        vmin = max(vmin, 0.7)
        vmax = min(vmax, 1.3)
    elif op == "contrast":
        vmin = max(vmin, 0.7)
        vmax = min(vmax, 1.3)
    elif op == "sharpness":
        vmin = max(vmin, 0.0)
        vmax = min(vmax, 1.5)
    elif op == "brightness":
        vmin = max(vmin, -80.0)
        vmax = min(vmax, 80.0)
    elif op == "equalization":
        vmin = max(vmin, 5.0)
        vmax = min(vmax, 40.0)

    return float(vmin), float(vmax)


def create_full_features_with_orig(df):
    """
    åŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(stepÃ—op) + å…ƒç”»åƒç‰¹å¾´(*_orig, *_orig_area) ã‚’ã¾ã¨ã‚ãŸç‰¹å¾´é‡è¡Œåˆ—ã‚’ä½œã‚‹ã€‚
    æˆ»ã‚Šå€¤: X_all, orig_cols
      - X_all: å…¨ç‰¹å¾´ DataFrame
      - orig_cols: ã€Œå…ƒç”»åƒç‰¹å¾´ã€ã®åˆ—åãƒªã‚¹ãƒˆï¼ˆæ–°ã—ã„ç”»åƒã«ã‚³ãƒ”ãƒ¼ã™ã‚‹å¯¾è±¡ï¼‰
    """
    X_param = create_interaction_features(df)

    orig_cols = [c for c in df.columns if c.endswith("_orig") or c.endswith("_orig_area")]
    if orig_cols:
        X_orig = df[orig_cols].copy()
    else:
        X_orig = pd.DataFrame(index=df.index)

    X_all = pd.concat([X_param, X_orig], axis=1)
    return X_all, orig_cols


# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª
# ==========================================
def main():
    st.set_page_config(page_title="ç”»åƒåŠ å·¥åˆ†æãƒ„ãƒ¼ãƒ«", layout="wide")
    st.title("ğŸ§ª ç”»åƒåŠ å·¥åˆ†æ & æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ‡ãƒ¼ã‚¿å…¥åŠ›
    st.sidebar.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.sidebar.file_uploader("å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿(CSV/Excel)", type=["csv", "xlsx", "xls"])

    if uploaded_file is None:
        st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    try:
        df_full = load_and_parse_data(uploaded_file)
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    sample_weights = compute_sample_weights(df_full)

    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(
        [
            "ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦",
            "ğŸ” ã‚¢ãƒ—ãƒ­ãƒ¼ãƒA: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ",
            "ğŸ¤– ã‚¢ãƒ—ãƒ­ãƒ¼ãƒB: å€‹åˆ¥MLåˆ†æ",
            "ğŸš€ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒC: MLåŒæ™‚æœ€é©åŒ–",
            "ğŸ† ã‚¢ãƒ—ãƒ­ãƒ¼ãƒD: 18ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ",
            "ğŸ§¬ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒE: ç”»åƒç‰¹å¾´ã«å¿œã˜ãŸåŠ å·¥æ¨è–¦",
        ]
    )

    # ---------------------------------------------------------------------
    # Tab 1: ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
    # ---------------------------------------------------------------------
    with tab1:
        st.subheader("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦")
        st.write(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: **{len(df_full)}** è¡Œ")
        st.dataframe(df_full.head())

        st.divider()
        st.subheader("å®Ÿé¨“ã•ã‚ŒãŸåŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ„ã¿åˆã‚ã›ä»¶æ•°")

        pattern_counts = df_full["pattern_id"].value_counts().sort_values(ascending=False)

        if not pattern_counts.empty:
            fig_height = max(5, len(pattern_counts) * 0.4)
            fig, ax = plt.subplots(figsize=(10, fig_height))
            bars = ax.barh(pattern_counts.index, pattern_counts.values)
            ax.set_xlabel("ä»¶æ•°")
            ax.set_title("åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†å¸ƒ")
            ax.grid(axis="x", linestyle="--", alpha=0.7)
            for bar in bars:
                width = bar.get_width()
                ax.text(
                    width + 1,
                    bar.get_y() + bar.get_height() / 2,
                    f"{int(width)}",
                    ha="left",
                    va="center",
                    fontweight="bold",
                )
            st.pyplot(fig)
        else:
            st.warning("ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        st.divider()
        st.subheader("param / param_val ã®åˆ†å¸ƒãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰")

        op_counts = pd.concat([df_full["param1"], df_full["param2"], df_full["param3"]]).value_counts().rename(
            "count"
        )
        st.markdown("å„åŠ å·¥ç¨®åˆ¥ã®å‡ºç¾å›æ•°ï¼ˆ3ã‚¹ãƒ†ãƒƒãƒ—åˆè¨ˆï¼‰")
        st.dataframe(op_counts.to_frame(), use_container_width=True)

        st.markdown(
            """
        **ğŸ”§ åã‚Šè£œæ­£ã®è€ƒãˆæ–¹ï¼ˆTab2ã€œ6 ã§å…±é€šï¼‰**  

        - å„ç”»åƒã« `pattern_id = param1 â†’ param2 â†’ param3` ã‚’ä»˜ä¸ã€‚  
        - `pattern_id` ã”ã¨ã®å‡ºç¾å›æ•°ã‚’æ•°ãˆã€ãã® **é€†æ•° (1 / å›æ•°)** ã‚’ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ã¨ã—ã¦å­¦ç¿’ã«ä½¿ç”¨ã€‚  
        - ã“ã‚Œã«ã‚ˆã‚Šã€é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘ã§ãªããƒ¬ã‚¢ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã€  
          ã§ãã‚‹ã ã‘ **å…¬å¹³ã«å­¦ç¿’ã¸å¯„ä¸** ã™ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚
        """
        )

    # ---------------------------------------------------------------------
    # Tab 2: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ
    # ---------------------------------------------------------------------
    with tab2:
        st.header("ğŸ” ã‚¢ãƒ—ãƒ­ãƒ¼ãƒA: å®Ÿç¸¾ã‹ã‚‰ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º")
        st.markdown("ç”»è³ªæŒ‡æ¨™ã®é–¾å€¤ã§ã€æˆåŠŸç”»åƒã€ã‚’å®šç¾©ã—ã€ãƒªãƒ•ãƒˆå€¤ã§åŠ å·¥ã®å‹ç‡ã‚’ã¿ã‚‹ã€‚")

        c1, c2, c3 = st.columns(3)
        q_mean = c1.slider("è¼åº¦(Mean) ä¸Šä½%", 0, 100, 30, 5) / 100.0
        q_entr = c2.slider("ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ ä¸Šä½%", 0, 100, 30, 5) / 100.0
        q_cont = c3.slider("ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ(RMS) ä¸‹ä½%", 0, 100, 30, 5) / 100.0

        th_mean = df_full["all_bL_mean"].quantile(1.0 - q_mean)
        th_entr = df_full["all_sh_grad_entropy"].quantile(1.0 - q_entr)
        th_cont = df_full["all_c_rms_contrast"].quantile(q_cont)

        success_df = df_full[
            (df_full["all_bL_mean"] >= th_mean)
            & (df_full["all_sh_grad_entropy"] >= th_entr)
            & (df_full["all_c_rms_contrast"] <= th_cont)
        ]

        st.metric("æ¡ä»¶ã‚’æº€ãŸã™æˆåŠŸç”»åƒæ•°", f"{len(success_df)} / {len(df_full)}")

        if len(success_df) > 0:
            st.divider()
            st.subheader("ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®å‹ç‡åˆ†æ (ãƒªãƒ•ãƒˆå€¤)")
            st.caption("ãƒªãƒ•ãƒˆå€¤ > 1.0 â†’ æˆåŠŸç‡ãŒå¹³å‡ã‚ˆã‚Šé«˜ã„")

            best_recipe_filter = {}
            cols = st.columns(3)

            for i in range(1, 4):
                with cols[i - 1]:
                    step_col = f"param{i}"
                    base = df_full[step_col].value_counts(normalize=True)
                    succ = success_df[step_col].value_counts(normalize=True)
                    lift = (succ / base).sort_values(ascending=False)

                    counts = df_full[step_col].value_counts()
                    valid_ops = counts[counts > 10].index
                    lift = lift.loc[lift.index.intersection(valid_ops)]

                    st.markdown(f"#### Step {i}")
                    if not lift.empty:
                        best_op = lift.idxmax()
                        best_recipe_filter[f"Step{i}"] = best_op
                        st.dataframe(
                            lift.to_frame(name="ãƒªãƒ•ãƒˆå€¤")
                            .style.format("{:.2f}")
                            .background_gradient(cmap="Reds"),
                            use_container_width=True,
                        )
                        st.success(f"æ¨å¥¨: **{best_op}**")
                    else:
                        st.warning("ãƒ‡ãƒ¼ã‚¿ä¸è¶³")

            st.divider()
            st.subheader("ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹æ¨å¥¨ãƒ¬ã‚·ãƒ”")

            rec_vals = []
            for i in range(1, 4):
                op = best_recipe_filter.get(f"Step{i}")
                if op:
                    vals = success_df.loc[success_df[f"param{i}"] == op, f"param{i}_val"]
                    avg = vals.mean() if not vals.empty else 0
                    rec_vals.append(f"{op} ({avg:.2f})")
                else:
                    rec_vals.append("N/A")

            st.info(f"ğŸ‘‰ **Step1:** {rec_vals[0]}  â†’  **Step2:** {rec_vals[1]}  â†’  **Step3:** {rec_vals[2]}")
        else:
            st.warning("æˆåŠŸç”»åƒãŒ0æšã§ã™ã€‚é–¾å€¤ã‚’ç·©ã‚ã¦ãã ã•ã„ã€‚")

    # ---------------------------------------------------------------------
    # Tab 3: å€‹åˆ¥MLåˆ†æ (æ„Ÿåº¦åˆ†æ)
    # ---------------------------------------------------------------------
    with tab3:
        st.header("ğŸ¤– ã‚¢ãƒ—ãƒ­ãƒ¼ãƒB: å€‹åˆ¥å› å­ã®æ„Ÿåº¦åˆ†æ")
        st.markdown(
            """
        3ã¤ã®ç›®çš„å¤‰æ•°ãã‚Œãã‚Œã«ã¤ã„ã¦ã€ç‹¬ç«‹ã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ã‚’æ§‹ç¯‰ã€‚  
        **pattern å‡ºç¾é »åº¦ã®é€†æ•°ã§ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ä»˜ã‘**ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚
        """
        )

        if st.button("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹ (æ„Ÿåº¦åˆ†æ)"):
            with st.spinner("ç‰¹å¾´é‡ç”Ÿæˆ & ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­..."):
                X = create_interaction_features(df_full)

                targets = {
                    "è¼åº¦ (bL_mean)": "all_bL_mean",
                    "ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (Entropy)": "all_sh_grad_entropy",
                    "ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ (RMS)": "all_c_rms_contrast",
                }

                for label, col_name in targets.items():
                    y = df_full[col_name]

                    X_tr, X_te, y_tr, y_te, w_tr, w_te = train_test_split(
                        X, y, sample_weights, test_size=0.2, random_state=42
                    )

                    rf = RandomForestRegressor(n_estimators=100, random_state=42)
                    rf.fit(X_tr, y_tr, sample_weight=w_tr)

                    r2_train = rf.score(X_tr, y_tr)
                    r2_test = rf.score(X_te, y_te)

                    st.divider()
                    st.markdown(f"#### ğŸ¯ {label}")
                    st.caption(f"[é‡ã¿ä»˜ã] Train $R^2$: **{r2_train:.3f}** / Test $R^2$: **{r2_test:.3f}**")

                    imps = rf.feature_importances_
                    feat_imp = (
                        pd.DataFrame({"feature": X.columns, "importance": imps})
                        .sort_values("importance", ascending=False)
                        .head(5)
                    )

                    corrs = []
                    dirs = []
                    for f in feat_imp["feature"]:
                        c = df_full[col_name].corr(X[f])
                        corrs.append(c)
                        dirs.append("â• å¢—åŠ " if c > 0 else "â– æ¸›å°‘")

                    feat_imp["Correlation"] = corrs
                    feat_imp["Direction"] = dirs

                    st.dataframe(
                        feat_imp[["feature", "importance", "Correlation", "Direction"]]
                        .style.background_gradient(subset=["importance"], cmap="Greens"),
                        use_container_width=True,
                    )
                    st.caption("â€» importance: å¯„ä¸åº¦ / Correlation: å€¤ã‚’ä¸Šã’ãŸã¨ãã®å¤‰åŒ–æ–¹å‘")

    # ---------------------------------------------------------------------
    # Tab 4: MLåŒæ™‚æœ€é©åŒ– (1ãƒ‘ã‚¿ãƒ¼ãƒ³ç”¨)
    # ---------------------------------------------------------------------
    with tab4:
        st.header("ğŸš€ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒC: MLã«ã‚ˆã‚‹å¤šç›®çš„æœ€é©åŒ– (1ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ·±æ˜ã‚Š)")
        st.markdown(
            """
        3ã¤ã®æŒ‡æ¨™ï¼ˆè¼åº¦ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼‰ã‚’**1ã¤ã®RFã§åŒæ™‚ã«å­¦ç¿’**ã—ï¼Œ  
        æŒ‡å®šã—ãŸ 1 ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã—ã¾ã™ã€‚
        """
        )

        c1, c2, c3 = st.columns(3)
        w_mean_c = c1.slider("è¼åº¦(Mean) é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w1_c")
        w_entr_c = c2.slider("Entropy é‡è¦–åº¦", 0.0, 5.0, 2.0, key="w2_c")
        w_cont_c = c3.slider("ContrastæŠ‘åˆ¶ é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w3_c")

        df_full["internal_pattern_id"] = df_full["pattern_id"].str.replace(" â†’ ", "_", regex=False)
        unique_patterns = sorted(df_full["internal_pattern_id"].unique().tolist())
        default_pattern = "gamma_sharpness_equalization"
        idx = unique_patterns.index(default_pattern) if default_pattern in unique_patterns else 0

        target_pattern = st.selectbox(
            "æœ€é©åŒ–ã—ãŸã„åŠ å·¥é †åºã‚’é¸æŠã—ã¦ãã ã•ã„", unique_patterns, index=idx, key="target_pattern_c"
        )
        st.markdown(f"é¸æŠä¸­ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: **{target_pattern.replace('_', ' â†’ ')}**")

        if st.button("ğŸš€ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (ã‚¢ãƒ—ãƒ­ãƒ¼ãƒC)"):
            with st.spinner("Multi-Output RF å­¦ç¿’ & ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒä¸­..."):
                X_all = create_interaction_features(df_full)
                y_cols = ["all_bL_mean", "all_sh_grad_entropy", "all_c_rms_contrast"]
                Y_all = df_full[y_cols]

                X_tr, X_te, Y_tr, Y_te, w_tr, w_te = train_test_split(
                    X_all, Y_all, sample_weights, test_size=0.2, random_state=42
                )

                rf_mo = RandomForestRegressor(n_estimators=100, random_state=42)
                rf_mo.fit(X_tr, Y_tr, sample_weight=w_tr)

                Y_pred_te = rf_mo.predict(X_te)
                r2_each = r2_score(Y_te, Y_pred_te, multioutput="raw_values")
                r2_mean = r2_score(Y_te, Y_pred_te, multioutput="uniform_average")

                labels = ["Mean", "Entropy", "Contrast"]
                r2_df = pd.DataFrame({"Metric": labels, "Test_R2": r2_each})
                st.subheader("Multi-Output RF ã®å½“ã¦ã¯ã¾ã‚Š (C)")
                st.dataframe(r2_df.style.format({"Test_R2": "{:.3f}"}), use_container_width=True)
                st.caption(f"å¹³å‡ Test RÂ²: **{r2_mean:.3f}**")

                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ
                n_trials = 10000
                sim_X = pd.DataFrame(0, index=range(n_trials), columns=X_all.columns)
                ops = target_pattern.split("_")
                sim_params = {}

                for i, op in enumerate(ops, 1):
                    vmin, vmax = get_param_range(df_full, i, op)
                    vals = np.random.uniform(vmin, vmax, n_trials)
                    col_name = f"step{i}_{op}"
                    if col_name in sim_X.columns:
                        sim_X[col_name] = vals
                    sim_params[f"Step{i} ({op})"] = vals

                preds_matrix = rf_mo.predict(sim_X)
                scaler = StandardScaler()
                metrics_norm = scaler.fit_transform(preds_matrix)

                scores = (
                    w_mean_c * metrics_norm[:, 0]
                    + w_entr_c * metrics_norm[:, 1]
                    - w_cont_c * metrics_norm[:, 2]
                )
                best_idx = np.argmax(scores)

                preds = {
                    "Mean": preds_matrix[:, 0],
                    "Entropy": preds_matrix[:, 1],
                    "Contrast": preds_matrix[:, 2],
                }

                st.divider()
                st.subheader("ğŸ‘‘ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒCã§å¾—ã‚‰ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
                for key, vals in sim_params.items():
                    st.write(f"**{key}:** `{vals[best_idx]:.3f}`")

                st.subheader("ğŸ“ˆ ãã®ã¨ãã®äºˆæ¸¬ç”»è³ªæŒ‡æ¨™")
                st.write(f"è¼åº¦ (Mean): **{preds['Mean'][best_idx]:.3f}**")
                st.write(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: **{preds['Entropy'][best_idx]:.3f}**")
                st.write(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: **{preds['Contrast'][best_idx]:.3f}**")

                # æ•£å¸ƒå›³
                st.subheader("è¨“ç·´åˆ†å¸ƒã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®æ¯”è¼ƒ")
                chart_df = pd.DataFrame(
                    {"Mean": preds["Mean"], "Entropy": preds["Entropy"], "Contrast": preds["Contrast"], "Score": scores}
                )
                top_k = chart_df.nlargest(100, "Score")

                fig, ax = plt.subplots(1, 2, figsize=(12, 5))
                ax[0].scatter(
                    df_full["all_bL_mean"],
                    df_full["all_c_rms_contrast"],
                    c="dimgray",
                    alpha=0.25,
                    s=3,
                    label="Train (å®Ÿãƒ‡ãƒ¼ã‚¿)",
                )
                ax[1].scatter(
                    df_full["all_sh_grad_entropy"],
                    df_full["all_c_rms_contrast"],
                    c="dimgray",
                    alpha=0.25,
                    s=3,
                    label="Train (å®Ÿãƒ‡ãƒ¼ã‚¿)",
                )

                ax[0].scatter(
                    chart_df["Mean"],
                    chart_df["Contrast"],
                    c="lightgray",
                    alpha=0.2,
                    s=2,
                    label="Sim (all)",
                )
                ax[1].scatter(
                    chart_df["Entropy"],
                    chart_df["Contrast"],
                    c="lightgray",
                    alpha=0.2,
                    s=2,
                    label="Sim (all)",
                )

                ax[0].scatter(
                    top_k["Mean"], top_k["Contrast"], c="red", alpha=0.9, s=15, label="Sim (Top Score)"
                )
                ax[1].scatter(
                    top_k["Entropy"], top_k["Contrast"], c="red", alpha=0.9, s=15, label="Sim (Top Score)"
                )

                ax[0].set_title("Mean vs Contrast")
                ax[0].set_xlabel("Mean")
                ax[0].set_ylabel("Contrast")
                ax[1].set_title("Entropy vs Contrast")
                ax[1].set_xlabel("Entropy")
                ax[1].set_ylabel("Contrast")

                for a in ax:
                    a.legend(loc="upper right", fontsize=8)

                st.pyplot(fig)

    # ---------------------------------------------------------------------
    # Tab 5: 18ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
    # ---------------------------------------------------------------------
    with tab5:
        st.header("ğŸ† ã‚¢ãƒ—ãƒ­ãƒ¼ãƒD: 18ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒæ¡ä»¶ã§æ¯”è¼ƒ")
        st.markdown(
            """
        - brightness / equalization ã®åˆ¶ç´„ä»˜ãã§å–ã‚Šã†ã‚‹ **18é€šã‚Šã®åŠ å·¥é †åº** ã‚’åˆ—æŒ™ã€‚  
        - å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ **åŒã˜è©¦è¡Œå›æ•°** ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ï¼Œ  
          MultiOutputRegressor ãŒäºˆæ¸¬ã—ãŸç”»è³ªæŒ‡æ¨™ã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã€‚  
        """
        )

        c1, c2, c3 = st.columns(3)
        w_mean_d = c1.slider("è¼åº¦(Mean) é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w1_d")
        w_entr_d = c2.slider("Entropy é‡è¦–åº¦", 0.0, 5.0, 2.0, key="w2_d")
        w_cont_d = c3.slider("ContrastæŠ‘åˆ¶ é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w3_d")

        n_trials_per_pattern = st.slider(
            "ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©¦è¡Œæ•°", min_value=200, max_value=5000, value=1000, step=200
        )

        if st.button("ğŸ 18ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€æ‹¬ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹"):
            with st.spinner("MultiOutputRegressor å­¦ç¿’ & 18ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€æ‹¬ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­..."):
                X_all = create_interaction_features(df_full)
                y_cols = ["all_bL_mean", "all_sh_grad_entropy", "all_c_rms_contrast"]
                Y_all = df_full[y_cols]

                X_tr, X_te, Y_tr, Y_te, w_tr, w_te = train_test_split(
                    X_all, Y_all, sample_weights, test_size=0.2, random_state=42
                )

                base_rf = RandomForestRegressor(n_estimators=150, random_state=42)
                mo = MultiOutputRegressor(base_rf)
                mo.fit(X_tr, Y_tr, sample_weight=w_tr)

                Y_pred_te = mo.predict(X_te)
                r2_each = r2_score(Y_te, Y_pred_te, multioutput="raw_values")
                r2_mean = r2_score(Y_te, Y_pred_te, multioutput="uniform_average")

                labels = ["Mean", "Entropy", "Contrast"]
                r2_df = pd.DataFrame({"Metric": labels, "Test_R2": r2_each})

                st.subheader("MultiOutputRegressor ã®å½“ã¦ã¯ã¾ã‚Š (D)")
                st.dataframe(r2_df.style.format({"Test_R2": "{:.3f}"}), use_container_width=True)
                st.caption(f"3æŒ‡æ¨™å¹³å‡ Test RÂ²: **{r2_mean:.3f}**")

                allowed_patterns = generate_allowed_patterns()
                st.markdown(f"æ¢ç´¢å¯¾è±¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: **{len(allowed_patterns)}** é€šã‚Š")

                sim_dfs = []

                for pat in allowed_patterns:
                    op1, op2, op3 = pat.split("_")
                    v1min, v1max = get_param_range(df_full, 1, op1)
                    v2min, v2max = get_param_range(df_full, 2, op2)
                    v3min, v3max = get_param_range(df_full, 3, op3)

                    vals1 = np.random.uniform(v1min, v1max, n_trials_per_pattern)
                    vals2 = np.random.uniform(v2min, v2max, n_trials_per_pattern)
                    vals3 = np.random.uniform(v3min, v3max, n_trials_per_pattern)

                    sim_X = pd.DataFrame(0, index=range(n_trials_per_pattern), columns=X_all.columns)
                    col1 = f"step1_{op1}"
                    col2 = f"step2_{op2}"
                    col3 = f"step3_{op3}"
                    if col1 in sim_X.columns:
                        sim_X[col1] = vals1
                    if col2 in sim_X.columns:
                        sim_X[col2] = vals2
                    if col3 in sim_X.columns:
                        sim_X[col3] = vals3

                    preds = mo.predict(sim_X)

                    df_pat = pd.DataFrame(
                        {
                            "pattern": pat,
                            "Mean": preds[:, 0],
                            "Entropy": preds[:, 1],
                            "Contrast": preds[:, 2],
                            "step1_op": op1,
                            "step2_op": op2,
                            "step3_op": op3,
                            "step1_val": vals1,
                            "step2_val": vals2,
                            "step3_val": vals3,
                        }
                    )
                    sim_dfs.append(df_pat)

                sim_all = pd.concat(sim_dfs, ignore_index=True)

                metrics_mat = sim_all[["Mean", "Entropy", "Contrast"]].values
                scaler = StandardScaler()
                metrics_norm = scaler.fit_transform(metrics_mat)

                sim_all["Score"] = (
                    w_mean_d * metrics_norm[:, 0]
                    + w_entr_d * metrics_norm[:, 1]
                    - w_cont_d * metrics_norm[:, 2]
                )

                def top5_mean(x):
                    k = max(1, int(len(x) * 0.05))
                    return x.nlargest(k).mean()

                summary = (
                    sim_all.groupby("pattern")["Score"].agg(max_score="max", top5_mean=top5_mean).reset_index()
                )
                summary = summary.sort_values(["top5_mean", "max_score"], ascending=False).reset_index(drop=True)

                st.subheader("18ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                st.dataframe(
                    summary.style.format({"max_score": "{:.3f}", "top5_mean": "{:.3f}"}), use_container_width=True
                )

                best_idx = sim_all["Score"].idxmax()
                best_row = sim_all.loc[best_idx]

                st.divider()
                st.subheader("ğŸ‘‘ å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸­ã®ç†æƒ³è§£å€™è£œï¼ˆScore æœ€å¤§ï¼‰")
                st.markdown(
                    f"- ãƒ‘ã‚¿ãƒ¼ãƒ³: **{best_row['pattern'].replace('_', ' â†’ ')}**  \n"
                    f"- Step1: **{best_row['step1_op']}** = `{best_row['step1_val']:.3f}`  \n"
                    f"- Step2: **{best_row['step2_op']}** = `{best_row['step2_val']:.3f}`  \n"
                    f"- Step3: **{best_row['step3_op']}** = `{best_row['step3_val']:.3f}`"
                )
                st.markdown("**ãã®ã¨ãã®äºˆæ¸¬ç”»è³ªæŒ‡æ¨™**")
                st.write(f"è¼åº¦ (Mean): **{best_row['Mean']:.3f}**")
                st.write(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: **{best_row['Entropy']:.3f}**")
                st.write(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: **{best_row['Contrast']:.3f}**")
                st.write(f"Score: **{best_row['Score']:.3f}**")

                st.subheader("è¨“ç·´åˆ†å¸ƒã¨18ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®æ¯”è¼ƒ")

                fig, ax = plt.subplots(1, 2, figsize=(12, 5))

                ax[0].scatter(
                    sim_all["Mean"],
                    sim_all["Contrast"],
                    c="lightgray",
                    alpha=0.15,
                    s=2,
                    label="Sim (18patterns all)",
                )
                ax[1].scatter(
                    sim_all["Entropy"],
                    sim_all["Contrast"],
                    c="lightgray",
                    alpha=0.15,
                    s=2,
                    label="Sim (18patterns all)",
                )

                ax[0].scatter(
                    df_full["all_bL_mean"],
                    df_full["all_c_rms_contrast"],
                    c="dimgray",
                    alpha=0.25,
                    s=3,
                    label="Train (real data)",
                )
                ax[1].scatter(
                    df_full["all_sh_grad_entropy"],
                    df_full["all_c_rms_contrast"],
                    c="dimgray",
                    alpha=0.25,
                    s=3,
                    label="Train (real data)",
                )

                ax[0].scatter(
                    best_row["Mean"], best_row["Contrast"], c="red", s=40, label="Best Score"
                )
                ax[1].scatter(
                    best_row["Entropy"], best_row["Contrast"], c="red", s=40, label="Best Score"
                )

                ax[0].set_title("Mean vs Contrast")
                ax[0].set_xlabel("Mean")
                ax[0].set_ylabel("Contrast")
                ax[1].set_title("Entropy vs Contrast")
                ax[1].set_xlabel("Entropy")
                ax[1].set_ylabel("Contrast")

                for a in ax:
                    a.legend(loc="upper right", fontsize=8)

                st.pyplot(fig)

    # ---------------------------------------------------------------------
    # Tab 6: ç”»åƒç‰¹å¾´ã«å¿œã˜ãŸåŠ å·¥æ¨è–¦ï¼ˆæ–°ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼‰
    # ---------------------------------------------------------------------
    with tab6:
        st.header("ğŸ§¬ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒE: ç”»åƒç‰¹å¾´ã«å¿œã˜ãŸåŠ å·¥æ¨è–¦")
        st.markdown(
            """
        **ç›®çš„**  
        ã‚‚ã¨ã‚‚ã¨ã®ç”»åƒç‰¹å¾´é‡ï¼ˆ`*_orig` / `*_orig_area`ï¼‰ã«å¿œã˜ã¦ã€  
        ã€Œã©ã®åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè‰¯ã•ãã†ã‹ã€ã‚’ MultiOutputRegressor ã§æ¨è–¦ã—ã¾ã™ã€‚

        **ã“ã“ã§ã¯** æ–°ã—ã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãã®å ´ã§ç‰¹å¾´é‡ã‚’è¨ˆç®—ã—ã¦  
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å…¥ã‚Œã€18ãƒ‘ã‚¿ãƒ¼ãƒ³Ã—ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ&Top3å€™è£œã®ç”»åƒã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
        """
        )

        X_all_tmp, orig_cols_tmp = create_full_features_with_orig(df_full)
        if not orig_cols_tmp:
            st.warning("ã“ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ *_orig / *_orig_area åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å…ƒç”»åƒç‰¹å¾´ã«å¿œã˜ãŸæ¨è–¦ã¯ã§ãã¾ã›ã‚“ã€‚")
            st.stop()

        st.subheader("1. å…¥åŠ›ã«ä½¿ã†å…ƒç”»åƒç‰¹å¾´é‡ã®é¸æŠ")
        selected_orig_cols = st.multiselect(
            "å…ƒç”»åƒç‰¹å¾´é‡ï¼ˆå…¥åŠ›ã«ä½¿ç”¨ï¼‰",
            options=orig_cols_tmp,
            default=orig_cols_tmp,
            help="ãƒã‚§ãƒƒã‚¯ã‚’å¤–ã—ãŸåˆ—ã¯ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™ã€‚",
        )
        if len(selected_orig_cols) == 0:
            st.error("å°‘ãªãã¨ã‚‚1åˆ—ã¯é¸ã‚“ã§ãã ã•ã„ã€‚")
            st.stop()

        st.subheader("2. ã‚¹ã‚³ã‚¢ã®é‡ã¿è¨­å®š")
        c1, c2, c3 = st.columns(3)
        w_mean_e = c1.slider("è¼åº¦(Mean) é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w1_e")
        w_entr_e = c2.slider("Entropy é‡è¦–åº¦", 0.0, 5.0, 2.0, key="w2_e")
        w_cont_e = c3.slider("ContrastæŠ‘åˆ¶ é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w3_e")

        st.subheader("3. æ–°ã—ã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        new_image_file = st.file_uploader("æ–°ã—ã„ç”»åƒ (JPEG/PNG)", type=["jpg", "jpeg", "png"], key="new_image_file")

        # fallback: è¨“ç·´ãƒ‡ãƒ¼ã‚¿1è¡Œã‚’ä½¿ã†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        def _fmt_idx(i):
            if "image_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'image_name']}"
            elif "file_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'file_name']}"
            else:
                return str(i)

        st.markdown("â€» ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãªã„å ´åˆã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®1è¡Œã‚’ã€æ–°ç”»åƒã€ã¨ã¿ãªã—ã¦è©¦ã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚")
        fallback_idx = st.selectbox(
            "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ã‚¹ãƒˆç”¨ã®1è¡Œã‚’é¸ã¶ï¼ˆæ–°ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡ã„å ´åˆç”¨ï¼‰",
            options=df_full.index,
            format_func=_fmt_idx,
        )

        n_trials_per_pattern_e = st.slider(
            "1ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚ãŸã‚Šã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒè©¦è¡Œæ•°", min_value=200, max_value=5000, value=1000, step=200
        )

        if st.button("ğŸš€ å­¦ç¿’ & æ¨è–¦åŠ å·¥ã®è¨ˆç®—"):
            with st.spinner("é †å•é¡Œãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ & æ¨è–¦åŠ å·¥ã®æ¢ç´¢ä¸­..."):

                # ---- é †å•é¡Œãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ----
                X_param = create_interaction_features(df_full)
                X_orig = df_full[selected_orig_cols].copy()
                X_all = pd.concat([X_param, X_orig], axis=1)
                orig_cols = selected_orig_cols

                y_cols = ["all_bL_mean", "all_sh_grad_entropy", "all_c_rms_contrast"]
                Y_all = df_full[y_cols]

                X_tr, X_te, Y_tr, Y_te, w_tr, w_te = train_test_split(
                    X_all, Y_all, sample_weights, test_size=0.2, random_state=42
                )

                base_rf = RandomForestRegressor(n_estimators=150, random_state=42, n_jobs=-1)
                mo = MultiOutputRegressor(base_rf)
                mo.fit(X_tr, Y_tr, sample_weight=w_tr)

                Y_pred_te = mo.predict(X_te)
                r2_each = r2_score(Y_te, Y_pred_te, multioutput="raw_values")
                r2_mean = r2_score(Y_te, Y_pred_te, multioutput="uniform_average")

                labels = ["Mean", "Entropy", "Contrast"]
                r2_df = pd.DataFrame({"Metric": labels, "Test_R2": r2_each})
                st.subheader("é †å•é¡Œãƒ¢ãƒ‡ãƒ«ã®å½“ã¦ã¯ã¾ã‚Š (E)")
                st.dataframe(r2_df.style.format({"Test_R2": "{:.3f}"}), use_container_width=True)
                st.caption(f"3æŒ‡æ¨™å¹³å‡ Test RÂ²: **{r2_mean:.3f}**")

                # ---- ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆ3å‡ºåŠ›ã®å¹³å‡ï¼‰ ----
                importances_list = [est.feature_importances_ for est in mo.estimators_]
                mean_importance = np.mean(importances_list, axis=0)
                imp_df = pd.DataFrame(
                    {
                        "feature": X_all.columns,
                        "importance": mean_importance,
                        "kind": ["orig_feature" if f in orig_cols else "param_feature" for f in X_all.columns],
                    }
                ).sort_values("importance", ascending=False)

                st.subheader("ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆ3å‡ºåŠ›ã®å¹³å‡ï¼‰")
                st.dataframe(
                    imp_df.head(30).style.background_gradient(subset=["importance"], cmap="Greens"),
                    use_container_width=True,
                )

                top_imp = imp_df.head(20).sort_values("importance")
                fig_imp, ax_imp = plt.subplots(figsize=(8, max(4, len(top_imp) * 0.3)))
                ax_imp.barh(top_imp["feature"], top_imp["importance"])
                ax_imp.set_xlabel("importance (avg over 3 outputs)")
                ax_imp.grid(axis="x", linestyle="--", alpha=0.6)
                st.pyplot(fig_imp)

                # ---- æ–°ç”»åƒã® *_orig ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ ----
                new_image_for_display = None

                if new_image_file is not None:
                    # PIL & BGR
                    pil_img = Image.open(new_image_file).convert("RGB")
                    new_image_for_display = pil_img
                    img_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
                    h, w = img_bgr.shape[:2]

                    # ROI + å…¨ä½“ã®ç‰¹å¾´é‡
                    roi_masks = fp.make_masks(h, w, SCREEN_W_MM, DIST_MM, RES_X, CENTER_DEG, PARAFOVEA_DEG)
                    feats_roi = fp.compute_features_for_image(
                        img_bgr,
                        roi_masks,
                        screen_w_mm=SCREEN_W_MM,
                        dist_mm=DIST_MM,
                        res_x=RES_X,
                    )
                    all_masks = fp.make_all_masks()
                    feats_all = fp.compute_features_for_image(
                        img_bgr,
                        all_masks,
                        screen_w_mm=SCREEN_W_MM,
                        dist_mm=DIST_MM,
                        res_x=RES_X,
                    )
                    feats_image = {**feats_roi, **feats_all}

                    total_px = float(h * w)
                    area_map = {name: float(mask.sum()) / total_px for name, mask in roi_masks.items()}
                    area_map["all"] = 1.0

                    orig_vec = pd.Series(index=orig_cols, dtype=float)
                    for c in orig_cols:
                        if c.endswith("_orig_area"):
                            region = c[:-10]
                            orig_vec[c] = area_map.get(region, np.nan)
                        elif c.endswith("_orig"):
                            base = c[:-5]  # ä¾‹: "all_bL_mean"
                            orig_vec[c] = feats_image.get(base, np.nan)
                        else:
                            orig_vec[c] = np.nan
                else:
                    # æ–°ç”»åƒãŒç„¡ã„å ´åˆã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿1è¡Œã‚’æµç”¨
                    orig_vec = df_full.loc[fallback_idx, orig_cols].astype(float)

                st.markdown("**ä½¿ç”¨ã™ã‚‹å…ƒç”»åƒç‰¹å¾´ã®ä¸€éƒ¨ï¼ˆå…ˆé ­10åˆ—ï¼‰**")
                st.write(orig_vec.head(10))

                # ---- 18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ ----
                allowed_patterns = generate_allowed_patterns()
                sim_dfs = []

                for pat in allowed_patterns:
                    op1, op2, op3 = pat.split("_")

                    v1min, v1max = get_param_range(df_full, 1, op1)
                    v2min, v2max = get_param_range(df_full, 2, op2)
                    v3min, v3max = get_param_range(df_full, 3, op3)

                    vals1 = np.random.uniform(v1min, v1max, n_trials_per_pattern_e)
                    vals2 = np.random.uniform(v2min, v2max, n_trials_per_pattern_e)
                    vals3 = np.random.uniform(v3min, v3max, n_trials_per_pattern_e)

                    sim_X = pd.DataFrame(0.0, index=range(n_trials_per_pattern_e), columns=X_all.columns)
                    for c in orig_cols:
                        sim_X[c] = orig_vec[c]

                    c1_name = f"step1_{op1}"
                    c2_name = f"step2_{op2}"
                    c3_name = f"step3_{op3}"
                    if c1_name in sim_X.columns:
                        sim_X[c1_name] = vals1
                    if c2_name in sim_X.columns:
                        sim_X[c2_name] = vals2
                    if c3_name in sim_X.columns:
                        sim_X[c3_name] = vals3

                    preds = mo.predict(sim_X)

                    df_pat = pd.DataFrame(
                        {
                            "pattern": pat,
                            "Mean": preds[:, 0],
                            "Entropy": preds[:, 1],
                            "Contrast": preds[:, 2],
                            "step1_op": op1,
                            "step2_op": op2,
                            "step3_op": op3,
                            "step1_val": vals1,
                            "step2_val": vals2,
                            "step3_val": vals3,
                        }
                    )
                    sim_dfs.append(df_pat)

                sim_all = pd.concat(sim_dfs, ignore_index=True)

                metrics_mat = sim_all[["Mean", "Entropy", "Contrast"]].values
                scaler = StandardScaler()
                metrics_norm = scaler.fit_transform(metrics_mat)

                sim_all["Score"] = (
                    w_mean_e * metrics_norm[:, 0]
                    + w_entr_e * metrics_norm[:, 1]
                    - w_cont_e * metrics_norm[:, 2]
                )

                def top5_mean(x):
                    k = max(1, int(len(x) * 0.05))
                    return x.nlargest(k).mean()

                summary_e = (
                    sim_all.groupby("pattern")["Score"].agg(max_score="max", top5_mean=top5_mean).reset_index()
                )
                summary_e = summary_e.sort_values(["top5_mean", "max_score"], ascending=False).reset_index(drop=True)

                st.subheader("ã“ã®ç”»åƒã«å¯¾ã™ã‚‹ 18ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©•ä¾¡ï¼ˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒEï¼‰")
                st.dataframe(
                    summary_e.style.format({"max_score": "{:.3f}", "top5_mean": "{:.3f}"}),
                    use_container_width=True,
                )

                st.subheader("Top 10 ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆtop5_mean é †ï¼‰ã®ã‚°ãƒ©ãƒ•")
                top10 = summary_e.head(10).copy()
                top10["pattern_disp"] = top10["pattern"].str.replace("_", " â†’ ")
                fig_pat, ax_pat = plt.subplots(figsize=(8, max(4, len(top10) * 0.4)))
                ax_pat.barh(top10["pattern_disp"], top10["top5_mean"])
                ax_pat.set_xlabel("top5_mean Score")
                ax_pat.invert_yaxis()
                ax_pat.grid(axis="x", linestyle="--", alpha=0.6)
                st.pyplot(fig_pat)

                # ---- Top3 å€™è£œã‚’å–å¾— ----
                best3 = sim_all.nlargest(3, "Score").reset_index(drop=True)
                best_row_e = best3.loc[0]

                st.divider()
                st.subheader("ğŸ‘‘ ã“ã®ç”»åƒã«å¯¾ã™ã‚‹ç†æƒ³è§£å€™è£œï¼ˆScore æœ€å¤§ï¼‰")
                st.markdown(
                    f"- ãƒ‘ã‚¿ãƒ¼ãƒ³: **{best_row_e['pattern'].replace('_', ' â†’ ')}**  \n"
                    f"- Step1: **{best_row_e['step1_op']}** = `{best_row_e['step1_val']:.3f}`  \n"
                    f"- Step2: **{best_row_e['step2_op']}** = `{best_row_e['step2_val']:.3f}`  \n"
                    f"- Step3: **{best_row_e['step3_op']}** = `{best_row_e['step3_val']:.3f}`"
                )
                st.markdown("**ãã®ã¨ãã®äºˆæ¸¬ç”»è³ªæŒ‡æ¨™**")
                st.write(f"è¼åº¦ (Mean): **{best_row_e['Mean']:.3f}**")
                st.write(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: **{best_row_e['Entropy']:.3f}**")
                st.write(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: **{best_row_e['Contrast']:.3f}**")
                st.write(f"Score: **{best_row_e['Score']:.3f}**")

                # ---- å…¥åŠ›ç”»åƒ & Top3 åŠ å·¥ç”»åƒã®è¡¨ç¤º ----
                if new_image_for_display is not None:
                    st.subheader("å…¥åŠ›ç”»åƒã¨æ¨å¥¨åŠ å·¥å¾Œã®ç”»åƒï¼ˆTop3ï¼‰")

                    st.image(new_image_for_display, caption="å…¥åŠ›ç”»åƒï¼ˆå…ƒç”»åƒï¼‰", use_container_width=True)

                    cols_img = st.columns(3)
                    for rank in range(len(best3)):
                        row = best3.loc[rank]
                        ops = [row["step1_op"], row["step2_op"], row["step3_op"]]
                        vals = [row["step1_val"], row["step2_val"], row["step3_val"]]
                        out_img = apply_processing_sequence(new_image_for_display, ops, vals)

                        with cols_img[rank]:
                            cap = (
                                f"Top{rank+1}: {row['pattern'].replace('_', ' â†’ ')}\n"
                                f"Score={row['Score']:.3f}"
                            )
                            st.image(out_img, caption=cap, use_container_width=True)
                else:
                    st.info("æ–°ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€Top3 æ¨å¥¨åŠ å·¥å¾Œã®ç”»åƒã‚‚è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

                # ---- åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ ----
                st.subheader("è¨“ç·´åˆ†å¸ƒã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®æ¯”è¼ƒï¼ˆã“ã®ç”»åƒå‘ã‘ï¼‰")

                fig_sc, ax_sc = plt.subplots(1, 2, figsize=(12, 5))

                ax_sc[0].scatter(
                    df_full["all_bL_mean"],
                    df_full["all_c_rms_contrast"],
                    c="dimgray",
                    alpha=0.25,
                    s=3,
                    label="Train (real)",
                )
                ax_sc[1].scatter(
                    df_full["all_sh_grad_entropy"],
                    df_full["all_c_rms_contrast"],
                    c="dimgray",
                    alpha=0.25,
                    s=3,
                    label="Train (real)",
                )

                ax_sc[0].scatter(
                    sim_all["Mean"],
                    sim_all["Contrast"],
                    c="lightgray",
                    alpha=0.15,
                    s=2,
                    label="Sim (all)",
                )
                ax_sc[1].scatter(
                    sim_all["Entropy"],
                    sim_all["Contrast"],
                    c="lightgray",
                    alpha=0.15,
                    s=2,
                    label="Sim (all)",
                )

                ax_sc[0].scatter(
                    best_row_e["Mean"], best_row_e["Contrast"], c="red", s=40, label="Best Score"
                )
                ax_sc[1].scatter(
                    best_row_e["Entropy"], best_row_e["Contrast"], c="red", s=40, label="Best Score"
                )

                ax_sc[0].set_title("Mean vs Contrast")
                ax_sc[0].set_xlabel("Mean")
                ax_sc[0].set_ylabel("Contrast")
                ax_sc[1].set_title("Entropy vs Contrast")
                ax_sc[1].set_xlabel("Entropy")
                ax_sc[1].set_ylabel("Contrast")

                for a in ax_sc:
                    a.legend(loc="upper right", fontsize=8)

                st.pyplot(fig_sc)


if __name__ == "__main__":
    main()

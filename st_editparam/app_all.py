import streamlit as st
import pandas as pd
import numpy as np

from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt


# ==========================================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & ãƒ‘ãƒ¼ã‚¹å‡¦ç† (å…±é€š)
# ==========================================
@st.cache_data
def load_and_parse_data(uploaded_file):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰é †åºã¨å€¤ã‚’æŠ½å‡ºã—ã¦æ§‹é€ åŒ–ã™ã‚‹"""
    # æ‹¡å¼µå­åˆ¤åˆ¥
    if uploaded_file.name.endswith('.csv'):
        df = pd.read_csv(uploaded_file)
    elif uploaded_file.name.endswith(('.xlsx', '.xls')):
        df = pd.read_excel(uploaded_file)
    else:
        df = pd.read_csv(uploaded_file)

    def parse_params_ordered(name):
        if pd.isna(name):
            return {
                'param1': 'None', 'param1_val': 0.0,
                'param2': 'None', 'param2_val': 0.0,
                'param3': 'None', 'param3_val': 0.0
            }

        clean_name = str(name).replace('.jpg', '').replace('.JPG', '')
        parts = clean_name.split('_')
        valid_ops = ['brightness', 'contrast', 'gamma', 'sharpness', 'equalization']

        params = []
        for part in parts:
            for op in valid_ops:
                if part.startswith(op):
                    try:
                        val_str = part.replace(op, '')
                        val = float(val_str)
                        params.append((op, val))
                    except ValueError:
                        continue
                    break

        # 3ã‚¹ãƒ†ãƒƒãƒ—æœªæº€ã‚’åŸ‹ã‚ã‚‹
        while len(params) < 3:
            params.append(('None', 0.0))

        return {
            'param1': params[0][0], 'param1_val': params[0][1],
            'param2': params[1][0], 'param2_val': params[1][1],
            'param3': params[2][0], 'param3_val': params[2][1]
        }

    parsed_list = [parse_params_ordered(n) for n in df['image_name']]
    params_df = pd.DataFrame(parsed_list)

    # é †åºãƒ‘ã‚¿ãƒ¼ãƒ³IDã‚’ä½œæˆ (ä¾‹: gamma -> sharpness -> equalization)
    params_df['pattern_id'] = (
        params_df['param1'] + " â†’ " + params_df['param2'] + " â†’ " + params_df['param3']
    )

    # é‡è¤‡åˆ—å‰Šé™¤
    cols_to_use = params_df.columns.tolist()
    df = df.drop(columns=[c for c in cols_to_use if c in df.columns], errors='ignore')

    df_full = pd.concat([df, params_df], axis=1)
    return df_full


# ==========================================
# 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° & é‡ã¿è¨ˆç®—
# ==========================================
def create_interaction_features(df):
    """
    'step1_gamma' ã®ã‚ˆã†ã«ã€ã€Œå ´æ‰€Ã—ç¨®é¡ã€ã§å€¤ã‚’æ ¼ç´ã™ã‚‹ç‰¹å¾´é‡ã‚’ä½œæˆã€‚
    ã“ã‚Œã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ã¯ã€Œ1æ‰‹ç›®ã®Gammaã€ã¨ã€Œ2æ‰‹ç›®ã®Gammaã€ã‚’åŒºåˆ¥ã§ãã‚‹ã€‚
    """
    valid_ops = ['brightness', 'contrast', 'gamma', 'sharpness', 'equalization']
    X_dict = {}

    for i in range(1, 4):
        col_op = f'param{i}'
        col_val = f'param{i}_val'

        for op in valid_ops:
            # è©²å½“ã™ã‚‹æ“ä½œã®å ´åˆã®ã¿å€¤ã‚’å…¥ã‚Œã€ãã‚Œä»¥å¤–ã¯0
            mask = (df[col_op] == op).astype(float)
            X_dict[f"step{i}_{op}"] = mask * df[col_val]

    return pd.DataFrame(X_dict, index=df.index)


def compute_sample_weights(df):
    """
    pattern_id ã”ã¨ã«ä»¶æ•°ã‚’æ•°ãˆã€ãã®é€†æ•°ã‚’é‡ã¿ã¨ã™ã‚‹ã€‚
    -> å¤šãå«ã¾ã‚Œã‚‹åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å­¦ç¿’ãŒåã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚
    """
    key = df['pattern_id']
    freq = key.value_counts()
    w = key.map(freq).astype(float)
    w = 1.0 / w
    # å¹³å‡1ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆä»»æ„ï¼‰
    w *= (len(w) / w.sum())
    return w


# ---- 18ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ & å€¤ã®ç¯„å›²å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ --------------------------
def generate_allowed_patterns():
    """
    brightnessã¯æœ€åˆ / equalizationã¯æœ€å¾Œ / brightnessã¨equalizationã¯åŒå±…ã—ãªã„ / é‡è¤‡ãªã—
    ã‚’æº€ãŸã™å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæƒ³å®š18é€šã‚Šï¼‰ã‚’åˆ—æŒ™ã€‚
    æ–‡å­—åˆ—è¡¨ç¾ã¯ 'op1_op2_op3'
    """
    ops = ["brightness", "contrast", "gamma", "sharpness", "equalization"]
    patterns = []

    for p1 in ops:
        for p2 in ops:
            for p3 in ops:
                pat = [p1, p2, p3]

                # é‡è¤‡ç¦æ­¢
                if len(set(pat)) < 3:
                    continue

                # brightness ã¯ã‚ã£ã¦ã‚‚ Step1 ã®ã¿
                if "brightness" in pat and p1 != "brightness":
                    continue

                # equalization ã¯ã‚ã£ã¦ã‚‚ Step3 ã®ã¿
                if "equalization" in pat and p3 != "equalization":
                    continue

                # brightness ã¨ equalization ã¯åŒå±…ã—ãªã„
                if "brightness" in pat and "equalization" in pat:
                    continue

                patterns.append(f"{p1}_{p2}_{p3}")

    return patterns  # 18å€‹ã«ãªã‚‹ã¯ãš


def get_param_range(df, step, op):
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ stepÃ—op ã”ã¨ã®å€¤ã® min/max ã‚’å–ã‚Šã€
    å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ç¯„å›²ã‚’è¿”ã™ã€‚
    """
    col_op = f'param{step}'
    col_val = f'param{step}_val'
    mask = df[col_op] == op

    if mask.any():
        vmin = df.loc[mask, col_val].min()
        vmax = df.loc[mask, col_val].max()
        if vmin == vmax:
            vmin -= abs(vmin) * 0.1 + 1e-3
            vmax += abs(vmax) * 0.1 + 1e-3
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç¯„å›²
        if op == 'gamma':
            vmin, vmax = 0.3, 1.5
        elif op == 'equalization':
            vmin, vmax = 5.0, 50.0
        elif op == 'brightness':
            vmin, vmax = -50, 50
        elif op == 'contrast':
            vmin, vmax = 0.5, 2.0
        else:  # sharpness ãªã©
            vmin, vmax = 0.0, 3.0

    return float(vmin), float(vmax)


def create_full_features_with_orig(df):
    """
    åŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(stepÃ—op) + å…ƒç”»åƒç‰¹å¾´(*_orig, *_orig_area) ã‚’ã¾ã¨ã‚ãŸç‰¹å¾´é‡è¡Œåˆ—ã‚’ä½œã‚‹ã€‚
    æˆ»ã‚Šå€¤: X_all, orig_cols
      - X_all: å…¨ç‰¹å¾´ DataFrame
      - orig_cols: ã€Œå…ƒç”»åƒç‰¹å¾´ã€ã®åˆ—åãƒªã‚¹ãƒˆï¼ˆæ–°ã—ã„ç”»åƒã«ã‚³ãƒ”ãƒ¼ã™ã‚‹å¯¾è±¡ï¼‰
    """
    # æ—¢å­˜ã® param ç‰¹å¾´
    X_param = create_interaction_features(df)

    # å…ƒç”»åƒç‰¹å¾´ï¼ˆ*_orig, *_orig_areaï¼‰ã‚’æ¢ã™
    orig_cols = [
        c for c in df.columns
        if c.endswith("_orig") or c.endswith("_orig_area")
    ]
    if orig_cols:
        X_orig = df[orig_cols].copy()
    else:
        X_orig = pd.DataFrame(index=df.index)

    X_all = pd.concat([X_param, X_orig], axis=1)
    return X_all, orig_cols


# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª
# ==========================================
def main():
    st.set_page_config(page_title="ç”»åƒåŠ å·¥åˆ†æãƒ„ãƒ¼ãƒ«", layout="wide")
    st.title("ğŸ§ª ç”»åƒåŠ å·¥åˆ†æ & æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ‡ãƒ¼ã‚¿å…¥åŠ›
    st.sidebar.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.sidebar.file_uploader("å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿(CSV/Excel)", type=["csv", "xlsx", "xls"])

    if uploaded_file is None:
        st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    try:
        df_full = load_and_parse_data(uploaded_file)
    except Exception as e:
        st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ï¼ˆpattern ã®åã‚Šè£œæ­£ç”¨ï¼‰
    sample_weights = compute_sample_weights(df_full)

    # ã‚¿ãƒ–å®šç¾© (6ã¤)
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦",
        "ğŸ” ã‚¢ãƒ—ãƒ­ãƒ¼ãƒA: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ",
        "ğŸ¤– ã‚¢ãƒ—ãƒ­ãƒ¼ãƒB: å€‹åˆ¥MLåˆ†æ",
        "ğŸš€ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒC: MLåŒæ™‚æœ€é©åŒ–",
        "ğŸ† ã‚¢ãƒ—ãƒ­ãƒ¼ãƒD: 18ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ",
        "ğŸ§¬ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒE: ç”»åƒç‰¹å¾´ã«å¿œã˜ãŸåŠ å·¥æ¨è–¦"
    ])

    # ---------------------------------------------------------------------
    # Tab 1: ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
    # ---------------------------------------------------------------------
    with tab1:
        st.subheader("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦")
        st.write(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: **{len(df_full)}** è¡Œ")
        st.dataframe(df_full.head())

        st.divider()
        st.subheader("å®Ÿé¨“ã•ã‚ŒãŸåŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ„ã¿åˆã‚ã›ä»¶æ•°")

        pattern_counts = df_full['pattern_id'].value_counts().sort_values(ascending=False)

        if not pattern_counts.empty:
            fig_height = max(5, len(pattern_counts) * 0.4)
            fig, ax = plt.subplots(figsize=(10, fig_height))
            bars = ax.barh(pattern_counts.index, pattern_counts.values)
            ax.set_xlabel("ä»¶æ•°")
            ax.set_title("åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†å¸ƒ")
            ax.grid(axis='x', linestyle='--', alpha=0.7)
            for bar in bars:
                width = bar.get_width()
                ax.text(width + 1, bar.get_y() + bar.get_height() / 2,
                        f'{int(width)}', ha='left', va='center', fontweight='bold')
            st.pyplot(fig)
        else:
            st.warning("ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        st.divider()
        st.subheader("param / param_val ã®åˆ†å¸ƒãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰")

        op_counts = pd.concat([
            df_full['param1'],
            df_full['param2'],
            df_full['param3']
        ]).value_counts().rename("count")

        st.markdown("å„åŠ å·¥ç¨®åˆ¥ã®å‡ºç¾å›æ•°ï¼ˆ3ã‚¹ãƒ†ãƒƒãƒ—åˆè¨ˆï¼‰")
        st.dataframe(op_counts.to_frame(), use_container_width=True)

        st.markdown("""
        **ğŸ”§ åã‚Šè£œæ­£ã®è€ƒãˆæ–¹ï¼ˆTab2ã€œ6 ã§å…±é€šï¼‰**  

        - å„ç”»åƒã« `pattern_id = param1 â†’ param2 â†’ param3` ã‚’ä»˜ä¸ã€‚  
        - `pattern_id` ã”ã¨ã®å‡ºç¾å›æ•°ã‚’æ•°ãˆã€ãã® **é€†æ•° (1 / å›æ•°)** ã‚’ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ã¨ã—ã¦å­¦ç¿’ã«ä½¿ç”¨ã€‚  
        - ã“ã‚Œã«ã‚ˆã‚Šã€é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘ã§ãªããƒ¬ã‚¢ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã€  
          ã§ãã‚‹ã ã‘ **å…¬å¹³ã«å­¦ç¿’ã¸å¯„ä¸** ã™ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚
        """)

    # ---------------------------------------------------------------------
    # Tab 2: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ
    # ---------------------------------------------------------------------
    with tab2:
        st.header("ğŸ” ã‚¢ãƒ—ãƒ­ãƒ¼ãƒA: å®Ÿç¸¾ã‹ã‚‰ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º")
        st.markdown("ç”»è³ªæŒ‡æ¨™ã®é–¾å€¤ã§ã€æˆåŠŸç”»åƒã€ã‚’å®šç¾©ã—ã€ãƒªãƒ•ãƒˆå€¤ã§åŠ å·¥ã®å‹ç‡ã‚’ã¿ã‚‹ã€‚")

        c1, c2, c3 = st.columns(3)
        q_mean = c1.slider("è¼åº¦(Mean) ä¸Šä½%", 0, 100, 30, 5) / 100.0
        q_entr = c2.slider("ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ ä¸Šä½%", 0, 100, 30, 5) / 100.0
        q_cont = c3.slider("ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ(RMS) ä¸‹ä½%", 0, 100, 30, 5) / 100.0

        th_mean = df_full['all_bL_mean'].quantile(1.0 - q_mean)
        th_entr = df_full['all_sh_grad_entropy'].quantile(1.0 - q_entr)
        th_cont = df_full['all_c_rms_contrast'].quantile(q_cont)

        success_df = df_full[
            (df_full['all_bL_mean'] >= th_mean) &
            (df_full['all_sh_grad_entropy'] >= th_entr) &
            (df_full['all_c_rms_contrast'] <= th_cont)
        ]

        st.metric("æ¡ä»¶ã‚’æº€ãŸã™æˆåŠŸç”»åƒæ•°", f"{len(success_df)} / {len(df_full)}")

        if len(success_df) > 0:
            st.divider()
            st.subheader("ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®å‹ç‡åˆ†æ (ãƒªãƒ•ãƒˆå€¤)")
            st.caption("ãƒªãƒ•ãƒˆå€¤ > 1.0 â†’ æˆåŠŸç‡ãŒå¹³å‡ã‚ˆã‚Šé«˜ã„")

            best_recipe_filter = {}
            cols = st.columns(3)

            for i in range(1, 4):
                with cols[i - 1]:
                    step_col = f'param{i}'
                    base = df_full[step_col].value_counts(normalize=True)
                    succ = success_df[step_col].value_counts(normalize=True)
                    lift = (succ / base).sort_values(ascending=False)

                    counts = df_full[step_col].value_counts()
                    valid_ops = counts[counts > 10].index
                    lift = lift.loc[lift.index.intersection(valid_ops)]

                    st.markdown(f"#### Step {i}")
                    if not lift.empty:
                        best_op = lift.idxmax()
                        best_recipe_filter[f'Step{i}'] = best_op
                        st.dataframe(
                            lift.to_frame(name="ãƒªãƒ•ãƒˆå€¤")
                            .style.format("{:.2f}")
                            .background_gradient(cmap="Reds"),
                            use_container_width=True
                        )
                        st.success(f"æ¨å¥¨: **{best_op}**")
                    else:
                        st.warning("ãƒ‡ãƒ¼ã‚¿ä¸è¶³")

            st.divider()
            st.subheader("ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹æ¨å¥¨ãƒ¬ã‚·ãƒ”")

            rec_vals = []
            for i in range(1, 4):
                op = best_recipe_filter.get(f'Step{i}')
                if op:
                    vals = success_df.loc[success_df[f'param{i}'] == op, f'param{i}_val']
                    avg = vals.mean() if not vals.empty else 0
                    rec_vals.append(f"{op} ({avg:.2f})")
                else:
                    rec_vals.append("N/A")

            st.info(f"ğŸ‘‰ **Step1:** {rec_vals[0]}  â†’  **Step2:** {rec_vals[1]}  â†’  **Step3:** {rec_vals[2]}")
        else:
            st.warning("æˆåŠŸç”»åƒãŒ0æšã§ã™ã€‚é–¾å€¤ã‚’ç·©ã‚ã¦ãã ã•ã„ã€‚")

    # ---------------------------------------------------------------------
    # Tab 3: å€‹åˆ¥MLåˆ†æ (æ„Ÿåº¦åˆ†æ)
    # ---------------------------------------------------------------------
    with tab3:
        st.header("ğŸ¤– ã‚¢ãƒ—ãƒ­ãƒ¼ãƒB: å€‹åˆ¥å› å­ã®æ„Ÿåº¦åˆ†æ")
        st.markdown("""
        3ã¤ã®ç›®çš„å¤‰æ•°ãã‚Œãã‚Œã«ã¤ã„ã¦ã€ç‹¬ç«‹ã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ã‚’æ§‹ç¯‰ã€‚  
        **pattern å‡ºç¾é »åº¦ã®é€†æ•°ã§ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ä»˜ã‘**ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚
        """)

        if st.button("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹ (æ„Ÿåº¦åˆ†æ)"):
            with st.spinner("ç‰¹å¾´é‡ç”Ÿæˆ & ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­..."):
                X = create_interaction_features(df_full)

                targets = {
                    'è¼åº¦ (bL_mean)': 'all_bL_mean',
                    'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (Entropy)': 'all_sh_grad_entropy',
                    'ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ (RMS)': 'all_c_rms_contrast'
                }

                for label, col_name in targets.items():
                    y = df_full[col_name]

                    X_tr, X_te, y_tr, y_te, w_tr, w_te = train_test_split(
                        X, y, sample_weights,
                        test_size=0.2, random_state=42
                    )

                    rf = RandomForestRegressor(n_estimators=100, random_state=42)
                    rf.fit(X_tr, y_tr, sample_weight=w_tr)

                    r2_train = rf.score(X_tr, y_tr)
                    r2_test = rf.score(X_te, y_te)

                    st.divider()
                    st.markdown(f"#### ğŸ¯ {label}")
                    st.caption(
                        f"[é‡ã¿ä»˜ã] Train $R^2$: **{r2_train:.3f}** / "
                        f"Test $R^2$: **{r2_test:.3f}**"
                    )

                    imps = rf.feature_importances_
                    feat_imp = (
                        pd.DataFrame({'feature': X.columns, 'importance': imps})
                        .sort_values('importance', ascending=False)
                        .head(5)
                    )

                    corrs = []
                    dirs = []
                    for f in feat_imp['feature']:
                        c = df_full[col_name].corr(X[f])
                        corrs.append(c)
                        dirs.append("â• å¢—åŠ " if c > 0 else "â– æ¸›å°‘")

                    feat_imp['Correlation'] = corrs
                    feat_imp['Direction'] = dirs

                    st.dataframe(
                        feat_imp[['feature', 'importance', 'Correlation', 'Direction']]
                        .style.background_gradient(subset=['importance'], cmap='Greens'),
                        use_container_width=True
                    )
                    st.caption("â€» importance: å¯„ä¸åº¦ / Correlation: å€¤ã‚’ä¸Šã’ãŸã¨ãã®å¤‰åŒ–æ–¹å‘")

    # ---------------------------------------------------------------------
    # Tab 4: MLåŒæ™‚æœ€é©åŒ– (Multi-Output RF 1ãƒ‘ã‚¿ãƒ¼ãƒ³ç”¨)
    # ---------------------------------------------------------------------
    with tab4:
        st.header("ğŸš€ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒC: MLã«ã‚ˆã‚‹å¤šç›®çš„æœ€é©åŒ– (1ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ·±æ˜ã‚Š)")
        st.markdown("""
        3ã¤ã®æŒ‡æ¨™ï¼ˆè¼åº¦ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼‰ã‚’**1ã¤ã®RFã§åŒæ™‚ã«å­¦ç¿’**ã—ï¼Œ  
        æŒ‡å®šã—ãŸ 1 ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã—ã¾ã™ã€‚
        """)

        c1, c2, c3 = st.columns(3)
        w_mean_c = c1.slider("è¼åº¦(Mean) é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w1_c")
        w_entr_c = c2.slider("Entropy é‡è¦–åº¦", 0.0, 5.0, 2.0, key="w2_c")
        w_cont_c = c3.slider("ContrastæŠ‘åˆ¶ é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w3_c")

        df_full['internal_pattern_id'] = df_full['pattern_id'].str.replace(' â†’ ', '_', regex=False)
        unique_patterns = sorted(df_full['internal_pattern_id'].unique().tolist())
        default_pattern = "gamma_sharpness_equalization"
        idx = unique_patterns.index(default_pattern) if default_pattern in unique_patterns else 0

        target_pattern = st.selectbox(
            "æœ€é©åŒ–ã—ãŸã„åŠ å·¥é †åºã‚’é¸æŠã—ã¦ãã ã•ã„",
            unique_patterns,
            index=idx,
            key="target_pattern_c"
        )
        st.markdown(f"é¸æŠä¸­ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: **{target_pattern.replace('_', ' â†’ ')}**")

        if st.button("ğŸš€ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (ã‚¢ãƒ—ãƒ­ãƒ¼ãƒC)"):
            with st.spinner("Multi-Output RF å­¦ç¿’ & ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒä¸­..."):
                X_all = create_interaction_features(df_full)
                y_cols = ['all_bL_mean', 'all_sh_grad_entropy', 'all_c_rms_contrast']
                Y_all = df_full[y_cols]

                X_tr, X_te, Y_tr, Y_te, w_tr, w_te = train_test_split(
                    X_all, Y_all, sample_weights,
                    test_size=0.2, random_state=42
                )

                rf_mo = RandomForestRegressor(n_estimators=100, random_state=42)
                rf_mo.fit(X_tr, Y_tr, sample_weight=w_tr)

                Y_pred_te = rf_mo.predict(X_te)
                r2_each = r2_score(Y_te, Y_pred_te, multioutput='raw_values')
                r2_mean = r2_score(Y_te, Y_pred_te, multioutput='uniform_average')

                labels = ['Mean', 'Entropy', 'Contrast']
                r2_df = pd.DataFrame({"Metric": labels, "Test_R2": r2_each})
                st.subheader("Multi-Output RF ã®å½“ã¦ã¯ã¾ã‚Š (C)")
                st.dataframe(r2_df.style.format({"Test_R2": "{:.3f}"}), use_container_width=True)
                st.caption(f"å¹³å‡ Test RÂ²: **{r2_mean:.3f}**")

                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒï¼ˆé¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ï¼‰
                n_trials = 10000
                sim_X = pd.DataFrame(0, index=range(n_trials), columns=X_all.columns)
                ops = target_pattern.split('_')
                sim_params = {}

                for i, op in enumerate(ops, 1):
                    vmin, vmax = get_param_range(df_full, i, op)
                    vals = np.random.uniform(vmin, vmax, n_trials)
                    col_name = f"step{i}_{op}"
                    if col_name in sim_X.columns:
                        sim_X[col_name] = vals
                    sim_params[f"Step{i} ({op})"] = vals

                preds_matrix = rf_mo.predict(sim_X)
                scaler = StandardScaler()
                metrics_norm = scaler.fit_transform(preds_matrix)

                scores = (
                    w_mean_c * metrics_norm[:, 0] +
                    w_entr_c * metrics_norm[:, 1] -
                    w_cont_c * metrics_norm[:, 2]
                )
                best_idx = np.argmax(scores)

                preds = {
                    'Mean': preds_matrix[:, 0],
                    'Entropy': preds_matrix[:, 1],
                    'Contrast': preds_matrix[:, 2]
                }

                st.divider()
                st.subheader("ğŸ‘‘ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒCã§å¾—ã‚‰ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
                for key, vals in sim_params.items():
                    st.write(f"**{key}:** `{vals[best_idx]:.3f}`")

                st.subheader("ğŸ“ˆ ãã®ã¨ãã®äºˆæ¸¬ç”»è³ªæŒ‡æ¨™")
                st.write(f"è¼åº¦ (Mean): **{preds['Mean'][best_idx]:.3f}**")
                st.write(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: **{preds['Entropy'][best_idx]:.3f}**")
                st.write(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: **{preds['Contrast'][best_idx]:.3f}**")

                # æ•£å¸ƒå›³
                st.subheader("è¨“ç·´åˆ†å¸ƒã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®æ¯”è¼ƒ")
                chart_df = pd.DataFrame({
                    'Mean': preds['Mean'],
                    'Entropy': preds['Entropy'],
                    'Contrast': preds['Contrast'],
                    'Score': scores
                })
                top_k = chart_df.nlargest(100, 'Score')

                fig, ax = plt.subplots(1, 2, figsize=(12, 5))

                # Train (æš—ã‚ã‚°ãƒ¬ãƒ¼)
                ax[0].scatter(df_full['all_bL_mean'], df_full['all_c_rms_contrast'],
                              c='dimgray', alpha=0.25, s=3, label="Train (å®Ÿãƒ‡ãƒ¼ã‚¿)")
                ax[1].scatter(df_full['all_sh_grad_entropy'], df_full['all_c_rms_contrast'],
                              c='dimgray', alpha=0.25, s=3, label="Train (å®Ÿãƒ‡ãƒ¼ã‚¿)")

                # Sim allï¼ˆè–„ã‚°ãƒ¬ãƒ¼ï¼‰
                ax[0].scatter(chart_df['Mean'], chart_df['Contrast'],
                              c='lightgray', alpha=0.2, s=2, label="Sim (all)")
                ax[1].scatter(chart_df['Entropy'], chart_df['Contrast'],
                              c='lightgray', alpha=0.2, s=2, label="Sim (all)")

                # Top scoreï¼ˆèµ¤ï¼‰
                ax[0].scatter(top_k['Mean'], top_k['Contrast'],
                              c='red', alpha=0.9, s=15, label="Sim (Top Score)")
                ax[1].scatter(top_k['Entropy'], top_k['Contrast'],
                              c='red', alpha=0.9, s=15, label="Sim (Top Score)")

                ax[0].set_title("Mean vs Contrast")
                ax[0].set_xlabel("Mean")
                ax[0].set_ylabel("Contrast")
                ax[1].set_title("Entropy vs Contrast")
                ax[1].set_xlabel("Entropy")
                ax[1].set_ylabel("Contrast")

                for a in ax:
                    a.legend(loc="upper right", fontsize=8)

                st.pyplot(fig)

    # ---------------------------------------------------------------------
    # Tab 5: 18ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒæ¡ä»¶ã§æ¯”è¼ƒï¼ˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒDï¼‰
    # ---------------------------------------------------------------------
    with tab5:
        st.header("ğŸ† ã‚¢ãƒ—ãƒ­ãƒ¼ãƒD: 18ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŒæ¡ä»¶ã§æ¯”è¼ƒ")
        st.markdown("""
        - brightness / equalization ã®åˆ¶ç´„ä»˜ãã§å–ã‚Šã†ã‚‹ **18é€šã‚Šã®åŠ å·¥é †åº** ã‚’åˆ—æŒ™ã€‚  
        - å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ **åŒã˜è©¦è¡Œå›æ•°** ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ï¼Œ  
          MultiOutputRegressor ãŒäºˆæ¸¬ã—ãŸç”»è³ªæŒ‡æ¨™ã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã€‚  
        - å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¤ã„ã¦  
          - `max_score` : æœ€å¤§ã‚¹ã‚³ã‚¢  
          - `top5_mean` : ä¸Šä½5%ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¹ã‚³ã‚¢å¹³å‡  
          ã‚’æŒ‡æ¨™ã¨ã—ã¦ãƒ©ãƒ³ã‚¯ä»˜ã‘ã—ã¾ã™ã€‚
        """)

        c1, c2, c3 = st.columns(3)
        w_mean_d = c1.slider("è¼åº¦(Mean) é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w1_d")
        w_entr_d = c2.slider("Entropy é‡è¦–åº¦", 0.0, 5.0, 2.0, key="w2_d")
        w_cont_d = c3.slider("ContrastæŠ‘åˆ¶ é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w3_d")

        n_trials_per_pattern = st.slider(
            "ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©¦è¡Œæ•°",
            min_value=200, max_value=5000, value=1000, step=200
        )

        if st.button("ğŸ 18ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€æ‹¬ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹"):
            with st.spinner("MultiOutputRegressor å­¦ç¿’ & 18ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€æ‹¬ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­..."):
                # ---- ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆé †å•é¡Œï¼‰ ----
                X_all = create_interaction_features(df_full)
                y_cols = ['all_bL_mean', 'all_sh_grad_entropy', 'all_c_rms_contrast']
                Y_all = df_full[y_cols]

                X_tr, X_te, Y_tr, Y_te, w_tr, w_te = train_test_split(
                    X_all, Y_all, sample_weights,
                    test_size=0.2, random_state=42
                )

                base_rf = RandomForestRegressor(n_estimators=150, random_state=42)
                mo = MultiOutputRegressor(base_rf)
                mo.fit(X_tr, Y_tr, sample_weight=w_tr)

                Y_pred_te = mo.predict(X_te)
                r2_each = r2_score(Y_te, Y_pred_te, multioutput='raw_values')
                r2_mean = r2_score(Y_te, Y_pred_te, multioutput='uniform_average')

                labels = ['Mean', 'Entropy', 'Contrast']
                r2_df = pd.DataFrame({"Metric": labels, "Test_R2": r2_each})

                st.subheader("MultiOutputRegressor ã®å½“ã¦ã¯ã¾ã‚Š (D)")
                st.dataframe(r2_df.style.format({"Test_R2": "{:.3f}"}), use_container_width=True)
                st.caption(f"3æŒ‡æ¨™å¹³å‡ Test RÂ²: **{r2_mean:.3f}**")

                # ---- 18ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€æ‹¬ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ----
                allowed_patterns = generate_allowed_patterns()
                st.markdown(f"æ¢ç´¢å¯¾è±¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: **{len(allowed_patterns)}** é€šã‚Š")

                sim_dfs = []

                for pat in allowed_patterns:
                    op1, op2, op3 = pat.split('_')
                    # ãã‚Œãã‚Œã®å€¤ã®ç¯„å›²ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã‚‹ï¼‰
                    v1min, v1max = get_param_range(df_full, 1, op1)
                    v2min, v2max = get_param_range(df_full, 2, op2)
                    v3min, v3max = get_param_range(df_full, 3, op3)

                    vals1 = np.random.uniform(v1min, v1max, n_trials_per_pattern)
                    vals2 = np.random.uniform(v2min, v2max, n_trials_per_pattern)
                    vals3 = np.random.uniform(v3min, v3max, n_trials_per_pattern)

                    sim_X = pd.DataFrame(0, index=range(n_trials_per_pattern), columns=X_all.columns)
                    col1 = f"step1_{op1}"
                    col2 = f"step2_{op2}"
                    col3 = f"step3_{op3}"
                    if col1 in sim_X.columns:
                        sim_X[col1] = vals1
                    if col2 in sim_X.columns:
                        sim_X[col2] = vals2
                    if col3 in sim_X.columns:
                        sim_X[col3] = vals3

                    preds = mo.predict(sim_X)

                    df_pat = pd.DataFrame({
                        "pattern": pat,
                        "Mean": preds[:, 0],
                        "Entropy": preds[:, 1],
                        "Contrast": preds[:, 2],
                        "step1_op": op1,
                        "step2_op": op2,
                        "step3_op": op3,
                        "step1_val": vals1,
                        "step2_val": vals2,
                        "step3_val": vals3,
                    })
                    sim_dfs.append(df_pat)

                sim_all = pd.concat(sim_dfs, ignore_index=True)

                # ---- ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¾ã¨ã‚ã¦æ¨™æº–åŒ–ï¼‰ ----
                metrics_mat = sim_all[["Mean", "Entropy", "Contrast"]].values
                scaler = StandardScaler()
                metrics_norm = scaler.fit_transform(metrics_mat)

                sim_all["Score"] = (
                    w_mean_d * metrics_norm[:, 0] +
                    w_entr_d * metrics_norm[:, 1] -
                    w_cont_d * metrics_norm[:, 2]
                )

                # ---- ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚° ----
                def top5_mean(x):
                    k = max(1, int(len(x) * 0.05))
                    return x.nlargest(k).mean()

                summary = (sim_all
                           .groupby("pattern")["Score"]
                           .agg(max_score="max", top5_mean=top5_mean)
                           .reset_index())

                summary = summary.sort_values(
                    ["top5_mean", "max_score"], ascending=False
                ).reset_index(drop=True)

                st.subheader("18ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                st.dataframe(
                    summary.style.format({"max_score": "{:.3f}", "top5_mean": "{:.3f}"}),
                    use_container_width=True
                )

                # ---- å…¨ä½“ã§ã®ã€Œç†æƒ³è§£å€™è£œã€ï¼ˆã‚¹ã‚³ã‚¢æœ€å¤§ï¼‰ ----
                best_idx = sim_all["Score"].idxmax()
                best_row = sim_all.loc[best_idx]

                st.divider()
                st.subheader("ğŸ‘‘ å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸­ã®ç†æƒ³è§£å€™è£œï¼ˆScore æœ€å¤§ï¼‰")

                st.markdown(
                    f"- ãƒ‘ã‚¿ãƒ¼ãƒ³: **{best_row['pattern'].replace('_', ' â†’ ')}**  \n"
                    f"- Step1: **{best_row['step1_op']}** = `{best_row['step1_val']:.3f}`  \n"
                    f"- Step2: **{best_row['step2_op']}** = `{best_row['step2_val']:.3f}`  \n"
                    f"- Step3: **{best_row['step3_op']}** = `{best_row['step3_val']:.3f}`"
                )

                st.markdown("**ãã®ã¨ãã®äºˆæ¸¬ç”»è³ªæŒ‡æ¨™**")
                st.write(f"è¼åº¦ (Mean): **{best_row['Mean']:.3f}**")
                st.write(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: **{best_row['Entropy']:.3f}**")
                st.write(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: **{best_row['Contrast']:.3f}**")
                st.write(f"Score: **{best_row['Score']:.3f}**")

                # ---- ã–ã£ãã‚Šæ•£å¸ƒå›³ï¼ˆTrain vs Simï¼‰----
                st.subheader("è¨“ç·´åˆ†å¸ƒã¨18ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®æ¯”è¼ƒ")

                fig, ax = plt.subplots(1, 2, figsize=(12, 5))

                # Sim (all)
                ax[0].scatter(sim_all['Mean'], sim_all['Contrast'],
                              c='lightgray', alpha=0.15, s=2, label="Sim (18patterns all)")
                ax[1].scatter(sim_all['Entropy'], sim_all['Contrast'],
                              c='lightgray', alpha=0.15, s=2, label="Sim (18patterns all)")

                # Train
                ax[0].scatter(df_full['all_bL_mean'], df_full['all_c_rms_contrast'],
                              c='dimgray', alpha=0.25, s=3, label="Train (real data)")
                ax[1].scatter(df_full['all_sh_grad_entropy'], df_full['all_c_rms_contrast'],
                              c='dimgray', alpha=0.25, s=3, label="Train (real data)")

                # Best 1ç‚¹
                ax[0].scatter(best_row['Mean'], best_row['Contrast'],
                              c='red', s=40, label="Best Score")
                ax[1].scatter(best_row['Entropy'], best_row['Contrast'],
                              c='red', s=40, label="Best Score")

                ax[0].set_title("Mean vs Contrast")
                ax[0].set_xlabel("Mean")
                ax[0].set_ylabel("Contrast")
                ax[1].set_title("Entropy vs Contrast")
                ax[1].set_xlabel("Entropy")
                ax[1].set_ylabel("Contrast")

                for a in ax:
                    a.legend(loc="upper right", fontsize=8)

                st.pyplot(fig)

        # ---------------------------------------------------------------------
    # Tab 6: ç”»åƒç‰¹å¾´ã«å¿œã˜ãŸåŠ å·¥æ¨è–¦ï¼ˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒEï¼‰
    # ---------------------------------------------------------------------
    with tab6:
        st.header("ğŸ§¬ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒE: ç”»åƒç‰¹å¾´ã«å¿œã˜ãŸåŠ å·¥æ¨è–¦")
        st.markdown("""
        **ç›®çš„**  
        ã‚‚ã¨ã‚‚ã¨ã®ç”»åƒç‰¹å¾´é‡ï¼ˆ`*_orig` / `*_orig_area`ï¼‰ã«å¿œã˜ã¦ã€  
        ã€Œã©ã®åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè‰¯ã•ãã†ã‹ã€ã‚’ MultiOutputRegressor ã§æ¨è–¦ã—ã¾ã™ã€‚

        æ‰‹é †:
        1. é †å•é¡Œãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’  
           - å…¥åŠ›: åŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(stepÃ—op) + å…ƒç”»åƒç‰¹å¾´(`*_orig`, `*_orig_area`)  
           - å‡ºåŠ›: Mean / Entropy / Contrast  
        2. æ–°ã—ã„ç”»åƒã®ç‰¹å¾´é‡ã‚’ 1 è¡Œã ã‘å…¥åŠ›  
        3. 18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ¯ã£ã¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—  
        4. ã‚¹ã‚³ã‚¢æœ€å¤§ã®åŠ å·¥ãƒ‘ã‚¿ãƒ¼ãƒ³ & ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨è–¦
        """)

        # --- å…ƒç”»åƒç‰¹å¾´ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ & å€™è£œåˆ—ã‚’å–å¾— -------------------
        X_all_tmp, orig_cols_tmp = create_full_features_with_orig(df_full)
        if not orig_cols_tmp:
            st.warning("ã“ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ *_orig / *_orig_area åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å…ƒç”»åƒç‰¹å¾´ã«å¿œã˜ãŸæ¨è–¦ã¯ã§ãã¾ã›ã‚“ã€‚")
            st.stop()

        st.subheader("1. å…¥åŠ›ã«ä½¿ã†å…ƒç”»åƒç‰¹å¾´é‡ã®é¸æŠ")
        st.caption("æ„å‘³ãŒã‚ã‚Šãã†ãªåˆ—ã ã‘ã‚’é¸ã‚“ã§å­¦ç¿’ã«ä½¿ãˆã¾ã™ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨ã¦ï¼‰ã€‚")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨åˆ—
        selected_orig_cols = st.multiselect(
            "å…ƒç”»åƒç‰¹å¾´é‡ï¼ˆå…¥åŠ›ã«ä½¿ç”¨ï¼‰",
            options=orig_cols_tmp,
            default=orig_cols_tmp,
            help="ãƒã‚§ãƒƒã‚¯ã‚’å¤–ã—ãŸåˆ—ã¯ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™ã€‚"
        )

        if len(selected_orig_cols) == 0:
            st.error("å°‘ãªãã¨ã‚‚1åˆ—ã¯é¸ã‚“ã§ãã ã•ã„ã€‚")
            st.stop()

        st.subheader("2. ã‚¹ã‚³ã‚¢ã®é‡ã¿è¨­å®š")
        c1, c2, c3 = st.columns(3)
        w_mean_e = c1.slider("è¼åº¦(Mean) é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w1_e")
        w_entr_e = c2.slider("Entropy é‡è¦–åº¦", 0.0, 5.0, 2.0, key="w2_e")
        w_cont_e = c3.slider("ContrastæŠ‘åˆ¶ é‡è¦–åº¦", 0.0, 5.0, 1.0, key="w3_e")

        st.subheader("3. æ–°ã—ã„ç”»åƒã®ç‰¹å¾´é‡ï¼ˆä»»æ„ï¼‰")
        st.markdown("original-only ã®ç‰¹å¾´é‡ã‚’ 1 è¡Œã ã‘æŒã¤ CSV / Excel ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆåˆ—åã¯ `*_orig`, `*_orig_area` ã«å¯¾å¿œï¼‰ã€‚")

        new_orig_file = st.file_uploader(
            "æ–°ã—ã„ç”»åƒã®ç‰¹å¾´é‡ (optional)",
            type=["csv", "xlsx", "xls"],
            key="new_orig_file"
        )

        # è¡¨ç¤ºç”¨ã« image_name ãŒã‚ã‚Œã°ä½¿ã†
        def _fmt_idx(i):
            if "image_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'image_name']}"
            elif "file_name" in df_full.columns:
                return f"{i}: {df_full.loc[i, 'file_name']}"
            else:
                return str(i)

        st.markdown("**ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¦ã„ãªã„å ´åˆ**ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã‹ã‚‰ 1 è¡Œé¸ã‚“ã§ \"æ–°ã—ã„ç”»åƒ\" ã¨ã¿ãªã—ã¦ãƒ†ã‚¹ãƒˆã§ãã¾ã™ã€‚")
        fallback_idx = st.selectbox(
            "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ã‚¹ãƒˆç”¨ã®1è¡Œã‚’é¸ã¶ï¼ˆæ–°ã—ã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡ã„å ´åˆç”¨ï¼‰",
            options=df_full.index,
            format_func=_fmt_idx
        )

        n_trials_per_pattern_e = st.slider(
            "1ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚ãŸã‚Šã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒè©¦è¡Œæ•°",
            min_value=200, max_value=5000, value=1000, step=200
        )

        if st.button("ğŸš€ å­¦ç¿’ & æ–°ã—ã„ç”»åƒã«å¯¾ã™ã‚‹æ¨è–¦åŠ å·¥ã‚’è¨ˆç®—"):
            with st.spinner("é †å•é¡Œãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ & æ¨è–¦åŠ å·¥ã®æ¢ç´¢ä¸­..."):

                # ---- é †å•é¡Œãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ----
                # é¸ã°ã‚ŒãŸå…ƒç”»åƒç‰¹å¾´ã®ã¿ã‚’ä½¿ã£ã¦ç‰¹å¾´é‡ã‚’æ§‹æˆ
                X_param = create_interaction_features(df_full)
                X_orig = df_full[selected_orig_cols].copy()
                X_all = pd.concat([X_param, X_orig], axis=1)
                orig_cols = selected_orig_cols  # å¾Œã§ã‚³ãƒ”ãƒ¼ã«ä½¿ã†åˆ—

                y_cols = ['all_bL_mean', 'all_sh_grad_entropy', 'all_c_rms_contrast']
                Y_all = df_full[y_cols]

                X_tr, X_te, Y_tr, Y_te, w_tr, w_te = train_test_split(
                    X_all, Y_all, sample_weights,
                    test_size=0.2, random_state=42
                )

                base_rf = RandomForestRegressor(
                    n_estimators=150,
                    random_state=42,
                    n_jobs=-1
                )
                mo = MultiOutputRegressor(base_rf)
                mo.fit(X_tr, Y_tr, sample_weight=w_tr)

                Y_pred_te = mo.predict(X_te)
                r2_each = r2_score(Y_te, Y_pred_te, multioutput='raw_values')
                r2_mean = r2_score(Y_te, Y_pred_te, multioutput='uniform_average')

                labels = ['Mean', 'Entropy', 'Contrast']
                r2_df = pd.DataFrame({"Metric": labels, "Test_R2": r2_each})
                st.subheader("é †å•é¡Œãƒ¢ãƒ‡ãƒ«ã®å½“ã¦ã¯ã¾ã‚Š (E)")
                st.dataframe(
                    r2_df.style.format({"Test_R2": "{:.3f}"}),
                    use_container_width=True
                )
                st.caption(f"3æŒ‡æ¨™å¹³å‡ Test RÂ²: **{r2_mean:.3f}**")

                # ---- ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆ3å‡ºåŠ›ã®å¹³å‡ï¼‰ -----------------------
                importances_list = []
                for est in mo.estimators_:
                    importances_list.append(est.feature_importances_)
                mean_importance = np.mean(importances_list, axis=0)

                imp_df = pd.DataFrame({
                    "feature": X_all.columns,
                    "importance": mean_importance,
                    "kind": ["orig_feature" if f in orig_cols else "param_feature"
                             for f in X_all.columns]
                }).sort_values("importance", ascending=False)

                st.subheader("ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆ3å‡ºåŠ›ã®å¹³å‡ï¼‰")
                st.dataframe(
                    imp_df.head(30)
                    .style.background_gradient(subset=['importance'], cmap='Greens'),
                    use_container_width=True
                )

                # ã‚°ãƒ©ãƒ•ï¼ˆä¸Šä½20ç‰¹å¾´ï¼‰
                top_imp = imp_df.head(20).sort_values("importance")
                fig_imp, ax_imp = plt.subplots(figsize=(8, max(4, len(top_imp) * 0.3)))
                ax_imp.barh(top_imp["feature"], top_imp["importance"])
                ax_imp.set_xlabel("importance (avg over 3 outputs)")
                ax_imp.grid(axis="x", linestyle="--", alpha=0.6)
                st.pyplot(fig_imp)

                # ---- æ–°ã—ã„ç”»åƒã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« orig_vec ã‚’ä½œã‚‹ ------------
                def _load_table(f):
                    if f.name.endswith(".csv"):
                        return pd.read_csv(f)
                    elif f.name.endswith((".xlsx", ".xls")):
                        return pd.read_excel(f)
                    else:
                        return pd.read_csv(f)

                if new_orig_file is not None:
                    new_df = _load_table(new_orig_file)
                    # orig_cols ã«å¯¾å¿œã™ã‚‹åˆ—ã ã‘å–ã‚Šå‡ºã—ã€è¶³ã‚Šãªã„åˆ—ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡ã§è£œå®Œ
                    orig_vec = pd.Series(index=orig_cols, dtype=float)
                    for c in orig_cols:
                        if c in new_df.columns:
                            orig_vec[c] = float(new_df.loc[0, c])
                        else:
                            orig_vec[c] = float(df_full[c].mean())
                else:
                    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ fallback_idx è¡Œã‚’ä½¿ã†
                    orig_vec = df_full.loc[fallback_idx, orig_cols].astype(float)

                st.markdown("**ä½¿ç”¨ã™ã‚‹å…ƒç”»åƒç‰¹å¾´ã®ä¸€éƒ¨ï¼ˆå…ˆé ­10åˆ—ï¼‰**")
                st.write(orig_vec.head(10))

                # ---- 18ãƒ‘ã‚¿ãƒ¼ãƒ³ Ã— ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒã§æ¨è–¦åŠ å·¥ã‚’æ¢ç´¢ ----
                allowed_patterns = generate_allowed_patterns()
                sim_dfs = []

                for pat in allowed_patterns:
                    op1, op2, op3 = pat.split('_')

                    v1min, v1max = get_param_range(df_full, 1, op1)
                    v2min, v2max = get_param_range(df_full, 2, op2)
                    v3min, v3max = get_param_range(df_full, 3, op3)

                    vals1 = np.random.uniform(v1min, v1max, n_trials_per_pattern_e)
                    vals2 = np.random.uniform(v2min, v2max, n_trials_per_pattern_e)
                    vals3 = np.random.uniform(v3min, v3max, n_trials_per_pattern_e)

                    # X_all ã¨åŒã˜åˆ—æ§‹é€ ã® DataFrame ã‚’ä½œã‚‹
                    sim_X = pd.DataFrame(0.0, index=range(n_trials_per_pattern_e), columns=X_all.columns)

                    # å…ƒç”»åƒç‰¹å¾´ã¯å…¨è¡ŒåŒã˜å€¤ã«ã™ã‚‹
                    for c in orig_cols:
                        sim_X[c] = orig_vec[c]

                    # åŠ å·¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒãƒˆ
                    c1_name = f"step1_{op1}"
                    c2_name = f"step2_{op2}"
                    c3_name = f"step3_{op3}"
                    if c1_name in sim_X.columns:
                        sim_X[c1_name] = vals1
                    if c2_name in sim_X.columns:
                        sim_X[c2_name] = vals2
                    if c3_name in sim_X.columns:
                        sim_X[c3_name] = vals3

                    preds = mo.predict(sim_X)

                    df_pat = pd.DataFrame({
                        "pattern": pat,
                        "Mean": preds[:, 0],
                        "Entropy": preds[:, 1],
                        "Contrast": preds[:, 2],
                        "step1_op": op1,
                        "step2_op": op2,
                        "step3_op": op3,
                        "step1_val": vals1,
                        "step2_val": vals2,
                        "step3_val": vals3,
                    })
                    sim_dfs.append(df_pat)

                sim_all = pd.concat(sim_dfs, ignore_index=True)

                # ---- ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¾ã¨ã‚ã¦æ¨™æº–åŒ–ï¼‰ ----
                metrics_mat = sim_all[["Mean", "Entropy", "Contrast"]].values
                scaler = StandardScaler()
                metrics_norm = scaler.fit_transform(metrics_mat)

                sim_all["Score"] = (
                    w_mean_e * metrics_norm[:, 0] +
                    w_entr_e * metrics_norm[:, 1] -
                    w_cont_e * metrics_norm[:, 2]
                )

                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®è©•ä¾¡ï¼ˆmax_score / top5_meanï¼‰
                def top5_mean(x):
                    k = max(1, int(len(x) * 0.05))
                    return x.nlargest(k).mean()

                summary_e = (sim_all
                             .groupby("pattern")["Score"]
                             .agg(max_score="max", top5_mean=top5_mean)
                             .reset_index())

                summary_e = summary_e.sort_values(
                    ["top5_mean", "max_score"], ascending=False
                ).reset_index(drop=True)

                st.subheader("ã“ã®ç”»åƒã«å¯¾ã™ã‚‹ 18ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©•ä¾¡ï¼ˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒEï¼‰")
                st.dataframe(
                    summary_e.style.format({"max_score": "{:.3f}", "top5_mean": "{:.3f}"}),
                    use_container_width=True
                )

                # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥è©•ä¾¡ã®ã‚°ãƒ©ãƒ•ï¼ˆTop 10ï¼‰
                st.subheader("Top 10 ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆtop5_mean é †ï¼‰ã®ã‚°ãƒ©ãƒ•")
                top10 = summary_e.head(10).copy()
                top10["pattern_disp"] = top10["pattern"].str.replace("_", " â†’ ")

                fig_pat, ax_pat = plt.subplots(figsize=(8, max(4, len(top10) * 0.4)))
                ax_pat.barh(top10["pattern_disp"], top10["top5_mean"])
                ax_pat.set_xlabel("top5_mean Score")
                ax_pat.invert_yaxis()
                ax_pat.grid(axis="x", linestyle="--", alpha=0.6)
                st.pyplot(fig_pat)

                # å…¨ä½“ã§ã®ãƒ™ã‚¹ãƒˆ1å€™è£œ
                best_idx_e = sim_all["Score"].idxmax()
                best_row_e = sim_all.loc[best_idx_e]

                st.divider()
                st.subheader("ğŸ‘‘ ã“ã®ç”»åƒã«å¯¾ã™ã‚‹ç†æƒ³è§£å€™è£œï¼ˆScore æœ€å¤§ï¼‰")

                st.markdown(
                    f"- ãƒ‘ã‚¿ãƒ¼ãƒ³: **{best_row_e['pattern'].replace('_', ' â†’ ')}**  \n"
                    f"- Step1: **{best_row_e['step1_op']}** = `{best_row_e['step1_val']:.3f}`  \n"
                    f"- Step2: **{best_row_e['step2_op']}** = `{best_row_e['step2_val']:.3f}`  \n"
                    f"- Step3: **{best_row_e['step3_op']}** = `{best_row_e['step3_val']:.3f}`"
                )
                st.markdown("**ãã®ã¨ãã®äºˆæ¸¬ç”»è³ªæŒ‡æ¨™**")
                st.write(f"è¼åº¦ (Mean): **{best_row_e['Mean']:.3f}**")
                st.write(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: **{best_row_e['Entropy']:.3f}**")
                st.write(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: **{best_row_e['Contrast']:.3f}**")
                st.write(f"Score: **{best_row_e['Score']:.3f}**")

                # ---- è¨“ç·´åˆ†å¸ƒ vs ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®ãƒ—ãƒ­ãƒƒãƒˆ ---------
                st.subheader("è¨“ç·´åˆ†å¸ƒã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å¸ƒã®æ¯”è¼ƒï¼ˆã“ã®ç”»åƒå‘ã‘ï¼‰")

                fig_sc, ax_sc = plt.subplots(1, 2, figsize=(12, 5))

                # Trainï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
                ax_sc[0].scatter(df_full['all_bL_mean'], df_full['all_c_rms_contrast'],
                                 c='dimgray', alpha=0.25, s=3, label="Train (real)")
                ax_sc[1].scatter(df_full['all_sh_grad_entropy'], df_full['all_c_rms_contrast'],
                                 c='dimgray', alpha=0.25, s=3, label="Train (real)")

                # Simï¼ˆå…¨ã‚µãƒ³ãƒ—ãƒ«ï¼‰
                ax_sc[0].scatter(sim_all['Mean'], sim_all['Contrast'],
                                 c='lightgray', alpha=0.15, s=2, label="Sim (all)")
                ax_sc[1].scatter(sim_all['Entropy'], sim_all['Contrast'],
                                 c='lightgray', alpha=0.15, s=2, label="Sim (all)")

                # Best 1ç‚¹
                ax_sc[0].scatter(best_row_e['Mean'], best_row_e['Contrast'],
                                 c='red', s=40, label="Best Score")
                ax_sc[1].scatter(best_row_e['Entropy'], best_row_e['Contrast'],
                                 c='red', s=40, label="Best Score")

                ax_sc[0].set_title("Mean vs Contrast")
                ax_sc[0].set_xlabel("Mean")
                ax_sc[0].set_ylabel("Contrast")
                ax_sc[1].set_title("Entropy vs Contrast")
                ax_sc[1].set_xlabel("Entropy")
                ax_sc[1].set_ylabel("Contrast")

                for a in ax_sc:
                    a.legend(loc="upper right", fontsize=8)

                st.pyplot(fig_sc)

if __name__ == "__main__":
    main()